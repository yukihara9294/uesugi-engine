#!/usr/bin/env python3
"""
å²¡å±±çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ« ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å²¡å±±çœŒã®è¦³å…‰ãƒ»äººå£ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆãƒ»æ”¿ç­–ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
"""
import requests
import json
import csv
from datetime import datetime
from pathlib import Path
import logging
import time
from urllib.parse import urlparse, urljoin
import os

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class OkayamaDataCollector:
    """å²¡å±±çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # å²¡å±±çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ã‚µã‚¤ãƒˆ
        self.base_url = "https://www.okayama-opendata.jp/ckan/api/3/action"
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.base_dir = Path(__file__).parent.parent
        self.data_dir = self.base_dir / "uesugi-engine-data" / "okayama"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.tourism_dir = self.data_dir / "tourism"
        self.population_dir = self.data_dir / "population"
        self.events_dir = self.data_dir / "events"
        self.policy_dir = self.data_dir / "policy"
        self.transport_dir = self.data_dir / "transport"
        
        for dir in [self.tourism_dir, self.population_dir, self.events_dir, 
                   self.policy_dir, self.transport_dir]:
            dir.mkdir(exist_ok=True)
            
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "total_datasets": 0,
            "downloaded_files": 0,
            "failed_downloads": 0
        }
        
    def search_packages(self, query="", rows=1000):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¤œç´¢"""
        url = f"{self.base_url}/package_search"
        params = {
            "q": query,
            "rows": rows,
            "start": 0
        }
        
        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if data["success"]:
                logger.info(f"æ¤œç´¢çµæœ: {data['result']['count']}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
                return data["result"]["results"]
            else:
                logger.error("æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return []
                
        except requests.RequestException as e:
            logger.error(f"APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            return []
            
    def get_package_details(self, package_id):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        url = f"{self.base_url}/package_show"
        params = {"id": package_id}
        
        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if data["success"]:
                return data["result"]
            return None
            
        except requests.RequestException as e:
            logger.error(f"è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ ({package_id}): {e}")
            return None
            
    def categorize_dataset(self, dataset):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        title = dataset.get("title", "").lower()
        notes = dataset.get("notes", "").lower()
        tags = [tag["name"] for tag in dataset.get("tags", [])]
        tags_str = " ".join(tags).lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
        tourism_keywords = ["è¦³å…‰", "å®¿æ³Š", "ãƒ›ãƒ†ãƒ«", "æ—…é¤¨", "è¦³å…‰å®¢", "è¦³å…‰æ–½è¨­", "æ¸©æ³‰"]
        population_keywords = ["äººå£", "ä¸–å¸¯", "ä½æ°‘", "å¹´é½¢", "é«˜é½¢è€…", "å°‘å­åŒ–"]
        event_keywords = ["ã‚¤ãƒ™ãƒ³ãƒˆ", "ç¥­ã‚Š", "å‚¬ã—", "è¡Œäº‹", "ãƒ•ã‚§ã‚¹ãƒ†ã‚£ãƒãƒ«", "ç¥­å…¸"]
        transport_keywords = ["äº¤é€š", "ãƒã‚¹", "é‰„é“", "é“è·¯", "é§è»Šå ´", "ç©ºæ¸¯"]
        
        # å„ªå…ˆåº¦é †ã«ãƒã‚§ãƒƒã‚¯
        for keyword in tourism_keywords:
            if keyword in title or keyword in notes or keyword in tags_str:
                return "tourism"
                
        for keyword in population_keywords:
            if keyword in title or keyword in notes or keyword in tags_str:
                return "population"
                
        for keyword in event_keywords:
            if keyword in title or keyword in notes or keyword in tags_str:
                return "events"
                
        for keyword in transport_keywords:
            if keyword in title or keyword in notes or keyword in tags_str:
                return "transport"
                
        # ãã®ä»–ã¯æ”¿ç­–é–¢é€£
        return "policy"
        
    def download_resource(self, resource, category_dir):
        """ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            url = resource.get("url", "")
            if not url:
                return None
                
            format_type = resource.get("format", "").upper()
            
            # å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if format_type not in ["CSV", "JSON", "XLS", "XLSX", "GEOJSON"]:
                logger.info(f"éå¯¾å¿œãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {format_type}")
                return None
                
            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            original_name = resource.get("name", "unknown")
            if url.endswith(('.csv', '.json', '.xls', '.xlsx', '.geojson')):
                filename = os.path.basename(urlparse(url).path)
            else:
                ext = format_type.lower()
                filename = f"{original_name}.{ext}"
                
            # æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«åã®å‡¦ç†
            filename = filename.replace('/', '_').replace('\\', '_')
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            
            file_path = category_dir / filename
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        
            self.stats["downloaded_files"] += 1
            logger.info(f"âœ… ä¿å­˜å®Œäº†: {filename}")
            
            # CSVã®æ–‡å­—ã‚³ãƒ¼ãƒ‰å¤‰æ›
            if format_type == "CSV":
                self.convert_csv_encoding(file_path)
                
            return file_path
            
        except Exception as e:
            self.stats["failed_downloads"] += 1
            logger.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
            
    def convert_csv_encoding(self, file_path):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’UTF-8ã«å¤‰æ›"""
        encodings = ['shift_jis', 'cp932', 'utf-8', 'utf-8-sig', 'euc-jp']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                    
                # UTF-8ã§ä¿å­˜ã—ç›´ã™
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                    
                logger.debug(f"æ–‡å­—ã‚³ãƒ¼ãƒ‰å¤‰æ›: {encoding} â†’ UTF-8")
                break
            except UnicodeDecodeError:
                continue
            except Exception as e:
                logger.warning(f"æ–‡å­—ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—: {e}")
                break
                
    def collect_all_data(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("="*60)
        logger.info("å²¡å±±çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        logger.info("="*60)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åé›†çµæœ
        category_results = {
            "tourism": [],
            "population": [],
            "events": [],
            "transport": [],
            "policy": []
        }
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—
        datasets = self.search_packages()
        self.stats["total_datasets"] = len(datasets)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ã«å‡¦ç†
        for dataset in datasets:
            try:
                # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š
                category = self.categorize_dataset(dataset)
                
                # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé¸æŠ
                category_dirs = {
                    "tourism": self.tourism_dir,
                    "population": self.population_dir,
                    "events": self.events_dir,
                    "transport": self.transport_dir,
                    "policy": self.policy_dir
                }
                
                target_dir = category_dirs.get(category, self.policy_dir)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
                dataset_info = {
                    "id": dataset.get("id"),
                    "name": dataset.get("name"),
                    "title": dataset.get("title"),
                    "organization": dataset.get("organization", {}).get("title", ""),
                    "category": category,
                    "resources": []
                }
                
                # ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                for resource in dataset.get("resources", []):
                    file_path = self.download_resource(resource, target_dir)
                    if file_path:
                        dataset_info["resources"].append({
                            "name": resource.get("name"),
                            "format": resource.get("format"),
                            "file_path": str(file_path)
                        })
                        
                    # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                    time.sleep(0.5)
                    
                if dataset_info["resources"]:
                    category_results[category].append(dataset_info)
                    
            except Exception as e:
                logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
                
        # ã‚µãƒãƒªãƒ¼ä¿å­˜
        self._save_summary(category_results)
        
        return category_results
        
    def _save_summary(self, results):
        """åé›†çµæœã®ã‚µãƒãƒªãƒ¼ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # çµ±è¨ˆæƒ…å ±ã‚µãƒãƒªãƒ¼
        summary = {
            "timestamp": datetime.now().isoformat(),
            "prefecture": "å²¡å±±çœŒ",
            "stats": {
                "total_datasets": self.stats["total_datasets"],
                "downloaded_files": self.stats["downloaded_files"],
                "failed_downloads": self.stats["failed_downloads"],
                "by_category": {}
            },
            "datasets": results
        }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        for category, datasets in results.items():
            summary["stats"]["by_category"][category] = {
                "datasets": len(datasets),
                "files": sum(len(ds["resources"]) for ds in datasets)
            }
            
        # JSONä¿å­˜
        json_path = self.data_dir / f"okayama_summary_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
            
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
        report_path = self.data_dir / f"okayama_report_{timestamp}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("å²¡å±±çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("="*60 + "\n")
            f.write(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now()}\n")
            f.write(f"ç·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {self.stats['total_datasets']}ä»¶\n")
            f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {self.stats['downloaded_files']}ãƒ•ã‚¡ã‚¤ãƒ«\n")
            f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {self.stats['failed_downloads']}ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
            
            f.write("ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥åé›†çµæœã€‘\n")
            for category, stats in summary["stats"]["by_category"].items():
                f.write(f"\n{category.upper()}:\n")
                f.write(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {stats['datasets']}ä»¶\n")
                f.write(f"  ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats['files']}ä»¶\n")
                
                # ä¸»ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒªã‚¹ãƒˆ
                if category in results and results[category]:
                    f.write("  ä¸»ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:\n")
                    for ds in results[category][:5]:  # æœ€åˆã®5ä»¶
                        f.write(f"    - {ds['title']}\n")
                        
        logger.info(f"\nâœ… åé›†å®Œäº†ï¼")
        logger.info(f"ã‚µãƒãƒªãƒ¼: {json_path}")
        logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        
        return json_path, report_path
        
    def collect_alternative_sources(self):
        """ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®åé›†ï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰"""
        logger.info("\nã€ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åé›†ã€‘")
        
        alternative_data = []
        
        # 1. e-Statï¼ˆæ”¿åºœçµ±è¨ˆï¼‰ã‹ã‚‰å²¡å±±çœŒãƒ‡ãƒ¼ã‚¿
        try:
            estat_api_key = os.environ.get("ESTAT_API_KEY", "")
            if estat_api_key:
                import sys
                sys.path.append(str(self.base_dir / "scripts"))
                from collect_estat_data_v2 import collect_prefecture_statistics
                
                okayama_stats = collect_prefecture_statistics("33", "å²¡å±±çœŒ")
                if okayama_stats:
                    alternative_data.append({
                        "source": "e-Stat",
                        "type": "statistics",
                        "data": okayama_stats
                    })
                    
        except Exception as e:
            logger.error(f"e-Statåé›†ã‚¨ãƒ©ãƒ¼: {e}")
            
        # 2. å›½åœŸæ•°å€¤æƒ…å ±ã‹ã‚‰åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
        try:
            # å…¬å…±æ–½è¨­ã€è¦³å…‰è³‡æºç­‰ã®ãƒ‡ãƒ¼ã‚¿
            kokudo_urls = {
                "public_facilities": "https://nlftp.mlit.go.jp/ksj/gml/datalist/KsjTmplt-P02-v2_0.html",
                "tourist_resources": "https://nlftp.mlit.go.jp/ksj/gml/datalist/KsjTmplt-P12-v2_0.html"
            }
            
            for data_type, url in kokudo_urls.items():
                alternative_data.append({
                    "source": "å›½åœŸæ•°å€¤æƒ…å ±",
                    "type": data_type,
                    "url": url,
                    "note": "æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¿…è¦"
                })
                
        except Exception as e:
            logger.error(f"å›½åœŸæ•°å€¤æƒ…å ±å‚ç…§ã‚¨ãƒ©ãƒ¼: {e}")
            
        # ä»£æ›¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’ä¿å­˜
        alt_file = self.data_dir / "alternative_data_sources.json"
        with open(alt_file, 'w', encoding='utf-8') as f:
            json.dump(alternative_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±: {alt_file}")
        
        return alternative_data


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\nğŸ›ï¸ å²¡å±±çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†")
    print("="*60)
    
    collector = OkayamaDataCollector()
    
    try:
        # ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿åé›†
        results = collector.collect_all_data()
        
        # ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚‚åé›†
        alternative_sources = collector.collect_alternative_sources()
        
        # çµæœè¡¨ç¤º
        print("\nâœ… åé›†å®Œäº†çµ±è¨ˆ:")
        print(f"- ç·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {collector.stats['total_datasets']}ä»¶")
        print(f"- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {collector.stats['downloaded_files']}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {collector.stats['failed_downloads']}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"\nãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {collector.data_dir}")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ
        print("\nã€ã‚«ãƒ†ã‚´ãƒªåˆ¥åé›†çµæœã€‘")
        for category, datasets in results.items():
            if datasets:
                print(f"{category}: {len(datasets)}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
                
    except Exception as e:
        logger.error(f"åé›†å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        
    print("\n" + "="*60)


if __name__ == "__main__":
    main()