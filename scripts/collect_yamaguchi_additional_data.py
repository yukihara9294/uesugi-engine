#!/usr/bin/env python3
"""
å±±å£çœŒè¿½åŠ ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
äº¤é€šï¼ˆGTFSï¼‰ã€åŒ»ç™‚æ–½è¨­ã€æ•™è‚²æ–½è¨­ã€é˜²ç½æ–½è¨­ç­‰ã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
"""
import requests
import json
import csv
from datetime import datetime
from pathlib import Path
import logging
import time
import zipfile
import os

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class YamaguchiAdditionalDataCollector:
    """å±±å£çœŒè¿½åŠ ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_url = "https://yamaguchi-opendata.jp/ckan/api/3/action"
        self.base_dir = Path(__file__).parent.parent
        self.data_dir = self.base_dir / "uesugi-engine-data" / "yamaguchi"
        
        # è¿½åŠ ã‚«ãƒ†ã‚´ãƒªç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.transport_dir = self.data_dir / "transport"
        self.medical_dir = self.data_dir / "medical"
        self.education_dir = self.data_dir / "education" 
        self.disaster_dir = self.data_dir / "disaster"
        self.facilities_dir = self.data_dir / "facilities"
        
        for dir in [self.transport_dir, self.medical_dir, self.education_dir, 
                    self.disaster_dir, self.facilities_dir]:
            dir.mkdir(parents=True, exist_ok=True)
            
        # GTFSã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.gtfs_dir = self.transport_dir / "gtfs"
        self.gtfs_dir.mkdir(exist_ok=True)
        
        # åé›†çµæœã‚’ä¿å­˜
        self.collection_results = {
            "timestamp": datetime.now().isoformat(),
            "transport": [],
            "medical": [],
            "education": [],
            "disaster": [],
            "facilities": []
        }
        
    def search_and_download(self, keywords, category, category_dir):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        all_results = []
        
        for keyword in keywords:
            logger.info(f"ğŸ” æ¤œç´¢ä¸­: {keyword}")
            url = f"{self.base_url}/package_search"
            params = {
                "q": keyword,
                "rows": 100
            }
            
            try:
                response = requests.get(url, params=params)
                response.raise_for_status()
                data = response.json()
                
                if data["success"] and data["result"]["results"]:
                    results = data["result"]["results"]
                    logger.info(f"  â†’ {len(results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                    
                    for package in results:
                        package_info = {
                            "id": package["id"],
                            "title": package["title"],
                            "organization": package.get("organization", {}).get("title", ""),
                            "resources": []
                        }
                        
                        # ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        for resource in package.get("resources", []):
                            if resource["format"].upper() in ["CSV", "JSON", "XLS", "XLSX", "ZIP"]:
                                file_path = self.download_resource(resource, category_dir)
                                if file_path:
                                    package_info["resources"].append({
                                        "name": resource["name"],
                                        "format": resource["format"],
                                        "file": str(file_path),
                                        "url": resource["url"]
                                    })
                                    
                                    # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å±•é–‹
                                    if resource["format"].upper() == "ZIP" and file_path.suffix == ".zip":
                                        self.extract_zip(file_path, category_dir)
                        
                        if package_info["resources"]:
                            all_results.append(package_info)
                            
                time.sleep(1)  # APIè² è·è»½æ¸›
                
            except Exception as e:
                logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({keyword}): {e}")
                
        self.collection_results[category] = all_results
        return all_results
        
    def download_resource(self, resource, category_dir):
        """ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            url = resource["url"]
            format = resource["format"].upper()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            filename = resource["name"].replace('/', '_').replace('\\', '_')
            if not filename.endswith(f'.{format.lower()}'):
                filename += f'.{format.lower()}'
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            file_path = category_dir / filename
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        
            logger.info(f"    âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {filename}")
            return file_path
            
        except Exception as e:
            logger.error(f"    âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
            
    def extract_zip(self, zip_path, extract_dir):
        """ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹ï¼ˆGTFSãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰"""
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                extract_path = extract_dir / zip_path.stem
                extract_path.mkdir(exist_ok=True)
                zip_ref.extractall(extract_path)
                logger.info(f"    ğŸ“¦ ZIPå±•é–‹å®Œäº†: {extract_path}")
                
                # GTFSãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
                gtfs_files = ['agency.txt', 'stops.txt', 'routes.txt', 'trips.txt', 'stop_times.txt']
                found_files = [f for f in gtfs_files if (extract_path / f).exists()]
                if found_files:
                    logger.info(f"    ğŸšŒ GTFSãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {', '.join(found_files)}")
                    
        except Exception as e:
            logger.error(f"    âŒ ZIPå±•é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            
    def collect_transport_data(self):
        """äº¤é€šé–¢é€£ãƒ‡ãƒ¼ã‚¿ã®åé›†"""
        logger.info("\n=== äº¤é€šãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        keywords = [
            "ãƒã‚¹", "é‰„é“", "äº¤é€š", "GTFS", "æ™‚åˆ»è¡¨", "è·¯ç·š",
            "ãƒã‚¹åœ", "é§…", "å…¬å…±äº¤é€š", "é‹è¡Œ", "äº¤é€šæ©Ÿé–¢"
        ]
        return self.search_and_download(keywords, "transport", self.transport_dir)
        
    def collect_medical_data(self):
        """åŒ»ç™‚æ–½è¨­ãƒ‡ãƒ¼ã‚¿ã®åé›†"""
        logger.info("\n=== åŒ»ç™‚æ–½è¨­ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        keywords = [
            "ç—…é™¢", "è¨ºç™‚æ‰€", "åŒ»ç™‚", "ã‚¯ãƒªãƒ‹ãƒƒã‚¯", "åŒ»ç™‚æ©Ÿé–¢",
            "è–¬å±€", "æ­¯ç§‘", "åŒ»é™¢", "æ•‘æ€¥", "åŒ»ç™‚æ–½è¨­"
        ]
        return self.search_and_download(keywords, "medical", self.medical_dir)
        
    def collect_education_data(self):
        """æ•™è‚²æ–½è¨­ãƒ‡ãƒ¼ã‚¿ã®åé›†"""
        logger.info("\n=== æ•™è‚²æ–½è¨­ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        keywords = [
            "å­¦æ ¡", "å°å­¦æ ¡", "ä¸­å­¦æ ¡", "é«˜æ ¡", "å¤§å­¦",
            "å¹¼ç¨šåœ’", "ä¿è‚²åœ’", "æ•™è‚²", "å­¦æ ¡ä¸€è¦§", "æ•™è‚²æ–½è¨­"
        ]
        return self.search_and_download(keywords, "education", self.education_dir)
        
    def collect_disaster_data(self):
        """é˜²ç½é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®åé›†"""
        logger.info("\n=== é˜²ç½ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        keywords = [
            "é¿é›£æ‰€", "é¿é›£å ´æ‰€", "é˜²ç½", "ç½å®³", "ç·Šæ€¥",
            "é˜²ç½æ–½è¨­", "é¿é›£æ–½è¨­", "é˜²ç½æ‹ ç‚¹", "AED", "æ¶ˆé˜²"
        ]
        return self.search_and_download(keywords, "disaster", self.disaster_dir)
        
    def collect_facilities_data(self):
        """å…¬å…±æ–½è¨­ãƒ‡ãƒ¼ã‚¿ã®åé›†"""
        logger.info("\n=== å…¬å…±æ–½è¨­ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        keywords = [
            "å…¬å…±æ–½è¨­", "å¸‚å½¹æ‰€", "ç”ºå½¹å ´", "å…¬æ°‘é¤¨", "å›³æ›¸é¤¨",
            "ä½“è‚²é¤¨", "æ–‡åŒ–æ–½è¨­", "ã‚¹ãƒãƒ¼ãƒ„æ–½è¨­", "ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚»ãƒ³ã‚¿ãƒ¼"
        ]
        return self.search_and_download(keywords, "facilities", self.facilities_dir)
        
    def check_existing_gtfs(self):
        """æ—¢å­˜ã®GTFSãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯"""
        logger.info("\n=== æ—¢å­˜GTFSãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯ ===")
        
        # åºƒå³¶ã®GTFSãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        hiroshima_gtfs = self.base_dir / "uesugi-engine-data" / "hiroshima" / "transport" / "bus" / "gtfs_extracted"
        if hiroshima_gtfs.exists():
            logger.info(f"âœ… åºƒå³¶GTFSãƒ‡ãƒ¼ã‚¿ç™ºè¦‹: {hiroshima_gtfs}")
            
        # å±±å£çœŒå†…ã®GTFSã‚’æ¢ã™
        yamaguchi_gtfs_files = list(self.transport_dir.rglob("*.txt"))
        if yamaguchi_gtfs_files:
            logger.info(f"âœ… å±±å£çœŒGTFSãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(yamaguchi_gtfs_files)}å€‹")
            for f in yamaguchi_gtfs_files[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
                logger.info(f"  - {f.name}")
                
    def save_collection_summary(self):
        """åé›†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜"""
        summary_path = self.data_dir / f"additional_data_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        stats = {
            "total_datasets": sum(len(v) for v in self.collection_results.values() if isinstance(v, list)),
            "by_category": {
                k: len(v) if isinstance(v, list) else 0 
                for k, v in self.collection_results.items() if k != "timestamp"
            }
        }
        self.collection_results["statistics"] = stats
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(self.collection_results, f, ensure_ascii=False, indent=2)
            
        logger.info(f"\nğŸ“Š åé›†ã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_path}")
        logger.info(f"   ç·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {stats['total_datasets']}")
        for category, count in stats["by_category"].items():
            logger.info(f"   - {category}: {count}ä»¶")
            
    def run(self):
        """å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("å±±å£çœŒè¿½åŠ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {self.data_dir}")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        self.check_existing_gtfs()
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿åé›†
        self.collect_transport_data()
        self.collect_medical_data()
        self.collect_education_data()
        self.collect_disaster_data()
        self.collect_facilities_data()
        
        # ã‚µãƒãƒªãƒ¼ä¿å­˜
        self.save_collection_summary()
        
        logger.info("\nâœ… å…¨ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†ï¼")


if __name__ == "__main__":
    collector = YamaguchiAdditionalDataCollector()
    collector.run()