#!/usr/bin/env python3
"""
6åœ°åŸŸçµ±åˆãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åºƒå³¶çœŒã€å±±å£çœŒã€ç¦å²¡çœŒã€å¤§é˜ªåºœã€æ±äº¬éƒ½ã€ï¼ˆå²¡å±±çœŒï¼‰ã®çµ±åˆãƒ‡ãƒ¼ã‚¿åé›†

é–‹ç™ºãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã«åŸºã¥ã6åœ°åŸŸ:
1. åºƒå³¶çœŒ - é–‹ç™ºæ‹ ç‚¹ã€åˆæœŸå®Ÿè£…æ¸ˆã¿
2. å±±å£çœŒ - éš£æ¥çœŒã¨ã—ã¦å„ªå…ˆåº¦é«˜
3. ç¦å²¡çœŒ - ä¹å·ã®æ‹ ç‚¹éƒ½å¸‚
4. å¤§é˜ªåºœ - é–¢è¥¿ã®ä¸­å¿ƒåœ°
5. æ±äº¬éƒ½ - é¦–éƒ½åœãƒ‡ãƒ¼ã‚¿
6. å²¡å±±çœŒ - ä¸­å›½åœ°æ–¹ã®ä¸»è¦éƒ½å¸‚ï¼ˆè¿½åŠ äºˆå®šï¼‰
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'backend', 'app'))

import asyncio
import logging
from datetime import datetime
from pathlib import Path
import json
import subprocess
import concurrent.futures
from typing import Dict, List, Any

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SixRegionsIntegratedCollector:
    """6åœ°åŸŸçµ±åˆãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent.parent
        self.data_dir = self.base_dir / "uesugi-engine-data"
        self.scripts_dir = self.base_dir / "scripts"
        
        # 6åœ°åŸŸã®å®šç¾©
        self.regions = {
            "hiroshima": {
                "name": "åºƒå³¶çœŒ",
                "code": "34",
                "scripts": [
                    "collect_hiroshima_opendata.py",
                    "collect_hiroshima_bus_gtfs.py"
                ],
                "priority": 1
            },
            "yamaguchi": {
                "name": "å±±å£çœŒ", 
                "code": "35",
                "scripts": [
                    "collect_yamaguchi_data_v2.py"
                ],
                "priority": 1
            },
            "fukuoka": {
                "name": "ç¦å²¡çœŒ",
                "code": "40", 
                "scripts": [
                    "collect_bodik_fukuoka_data.py"
                ],
                "priority": 2
            },
            "osaka": {
                "name": "å¤§é˜ªåºœ",
                "code": "27",
                "scripts": [
                    "collect_major_cities_opendata.py"  # å¤§é˜ªå«ã‚€
                ],
                "priority": 2
            },
            "tokyo": {
                "name": "æ±äº¬éƒ½",
                "code": "13",
                "scripts": [
                    "collect_major_cities_opendata.py"  # æ±äº¬å«ã‚€
                ],
                "priority": 2
            },
            "okayama": {
                "name": "å²¡å±±çœŒ",
                "code": "33",
                "scripts": [
                    # å²¡å±±å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æœªä½œæˆ
                ],
                "priority": 3
            }
        }
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.integrated_dir = self.data_dir / "6regions_integrated"
        self.integrated_dir.mkdir(parents=True, exist_ok=True)
        
    def run_collection_script(self, script_name: str, region: str) -> Dict[str, Any]:
        """å€‹åˆ¥ã®åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ"""
        script_path = self.scripts_dir / script_name
        
        if not script_path.exists():
            logger.warning(f"ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {script_path}")
            return {"status": "not_found", "region": region, "script": script_name}
            
        try:
            logger.info(f"å®Ÿè¡Œä¸­: {script_name} ({region})")
            
            # Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
            result = subprocess.run(
                [sys.executable, str(script_path)],
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            
            if result.returncode == 0:
                logger.info(f"âœ… å®Œäº†: {script_name}")
                return {
                    "status": "success",
                    "region": region,
                    "script": script_name,
                    "output": result.stdout
                }
            else:
                logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {script_name}")
                return {
                    "status": "error",
                    "region": region,
                    "script": script_name,
                    "error": result.stderr
                }
                
        except subprocess.TimeoutExpired:
            logger.error(f"â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {script_name}")
            return {
                "status": "timeout",
                "region": region,
                "script": script_name
            }
        except Exception as e:
            logger.error(f"ä¾‹å¤–ç™ºç”Ÿ: {script_name} - {str(e)}")
            return {
                "status": "exception",
                "region": region,
                "script": script_name,
                "error": str(e)
            }
            
    def collect_region_data(self, region_key: str, region_info: Dict) -> List[Dict]:
        """åœ°åŸŸã”ã¨ã®ãƒ‡ãƒ¼ã‚¿åé›†"""
        results = []
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ã€{region_info['name']}ã€‘ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        logger.info(f"{'='*60}")
        
        if not region_info["scripts"]:
            # å²¡å±±çœŒãªã©ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒãªã„å ´åˆã¯åŸºæœ¬ãƒ‡ãƒ¼ã‚¿åé›†
            result = self.collect_basic_opendata(region_key, region_info)
            results.append(result)
        else:
            # æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
            for script in region_info["scripts"]:
                result = self.run_collection_script(script, region_key)
                results.append(result)
                
        return results
        
    def collect_basic_opendata(self, region_key: str, region_info: Dict) -> Dict:
        """åŸºæœ¬çš„ãªã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒãªã„åœ°åŸŸç”¨ï¼‰"""
        logger.info(f"{region_info['name']}ã®åŸºæœ¬ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å®Ÿè¡Œ")
        
        # e-Stat APIã‚’ä½¿ç”¨ã—ã¦åŸºæœ¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        try:
            import requests
            
            # äººå£ãƒ‡ãƒ¼ã‚¿ã®ä¾‹
            estat_url = "https://api.e-stat.go.jp/rest/3.0/app/json/getStatsData"
            params = {
                "appId": os.environ.get("ESTAT_API_KEY", ""),
                "statsDataId": "0003411825",  # äººå£æ¨è¨ˆ
                "cdArea": region_info["code"] + "000"  # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰
            }
            
            if params["appId"]:
                response = requests.get(estat_url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    
                    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
                    output_dir = self.data_dir / region_key / "statistics"
                    output_dir.mkdir(parents=True, exist_ok=True)
                    
                    output_file = output_dir / f"population_{datetime.now().strftime('%Y%m%d')}.json"
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                        
                    return {
                        "status": "success",
                        "region": region_key,
                        "type": "basic_statistics",
                        "file": str(output_file)
                    }
                    
            return {
                "status": "no_api_key",
                "region": region_key,
                "message": "e-Stat APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "region": region_key,
                "error": str(e)
            }
            
    def collect_common_data(self):
        """å…¨åœ°åŸŸå…±é€šãƒ‡ãƒ¼ã‚¿ã®åé›†"""
        logger.info("\nã€å…¨åœ°åŸŸå…±é€šãƒ‡ãƒ¼ã‚¿åé›†ã€‘")
        
        common_scripts = [
            "collect_estat_data_v2.py",  # æ”¿åºœçµ±è¨ˆ
            "collect_plateau_data.py",    # 3Déƒ½å¸‚ãƒ¢ãƒ‡ãƒ«
            "integrate_gbizinfo_api.py",  # ä¼æ¥­æƒ…å ±
            "collect_odpt_data.py"        # å…¬å…±äº¤é€š
        ]
        
        results = []
        for script in common_scripts:
            if (self.scripts_dir / script).exists():
                result = self.run_collection_script(script, "common")
                results.append(result)
                
        return results
        
    def integrate_collected_data(self, all_results: Dict) -> Dict:
        """åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        logger.info("\nã€ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ã€‘")
        
        integration_summary = {
            "timestamp": datetime.now().isoformat(),
            "regions": {},
            "common_data": {},
            "statistics": {
                "total_regions": len(self.regions),
                "successful_collections": 0,
                "failed_collections": 0,
                "total_files": 0
            }
        }
        
        # åœ°åŸŸåˆ¥ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
        for region, results in all_results["regions"].items():
            region_summary = {
                "name": self.regions[region]["name"],
                "code": self.regions[region]["code"],
                "collections": []
            }
            
            for result in results:
                if result["status"] == "success":
                    integration_summary["statistics"]["successful_collections"] += 1
                    region_summary["collections"].append({
                        "type": result.get("script", result.get("type", "unknown")),
                        "status": "success"
                    })
                else:
                    integration_summary["statistics"]["failed_collections"] += 1
                    region_summary["collections"].append({
                        "type": result.get("script", result.get("type", "unknown")),
                        "status": result["status"],
                        "error": result.get("error", "")
                    })
                    
            integration_summary["regions"][region] = region_summary
            
        # å…±é€šãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
        for result in all_results["common"]:
            data_type = result.get("script", "unknown").replace(".py", "")
            integration_summary["common_data"][data_type] = {
                "status": result["status"],
                "error": result.get("error", "") if result["status"] != "success" else None
            }
            
        # çµ±åˆçµæœã‚’ä¿å­˜
        summary_file = self.integrated_dir / f"integration_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(integration_summary, f, ensure_ascii=False, indent=2)
            
        logger.info(f"çµ±åˆã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_file}")
        
        return integration_summary
        
    def generate_unified_database_schema(self):
        """çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆ"""
        logger.info("\nã€çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆã€‘")
        
        schema_sql = """
-- 6åœ°åŸŸçµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒ
-- Generated: {timestamp}

-- åœ°åŸŸãƒã‚¹ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS regions (
    region_code VARCHAR(2) PRIMARY KEY,
    region_name VARCHAR(50) NOT NULL,
    region_name_en VARCHAR(50),
    priority INTEGER DEFAULT 3
);

-- åœ°åŸŸãƒ‡ãƒ¼ã‚¿åˆæœŸåŒ–
INSERT INTO regions (region_code, region_name, region_name_en, priority) VALUES
    ('34', 'åºƒå³¶çœŒ', 'Hiroshima', 1),
    ('35', 'å±±å£çœŒ', 'Yamaguchi', 1),
    ('40', 'ç¦å²¡çœŒ', 'Fukuoka', 2),
    ('27', 'å¤§é˜ªåºœ', 'Osaka', 2),
    ('13', 'æ±äº¬éƒ½', 'Tokyo', 2),
    ('33', 'å²¡å±±çœŒ', 'Okayama', 3)
ON CONFLICT (region_code) DO NOTHING;

-- çµ±åˆäººå£ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS integrated_population (
    id SERIAL PRIMARY KEY,
    region_code VARCHAR(2) REFERENCES regions(region_code),
    city_code VARCHAR(5),
    city_name VARCHAR(100),
    population INTEGER,
    households INTEGER,
    data_year INTEGER,
    data_month INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- çµ±åˆè¦³å…‰ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS integrated_tourism (
    id SERIAL PRIMARY KEY,
    region_code VARCHAR(2) REFERENCES regions(region_code),
    facility_name VARCHAR(200),
    facility_type VARCHAR(50),
    address TEXT,
    latitude DECIMAL(10, 8),
    longitude DECIMAL(11, 8),
    visitor_count INTEGER,
    data_year INTEGER,
    data_month INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- çµ±åˆã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS integrated_events (
    id SERIAL PRIMARY KEY,
    region_code VARCHAR(2) REFERENCES regions(region_code),
    event_name VARCHAR(200),
    event_type VARCHAR(50),
    venue VARCHAR(200),
    start_date DATE,
    end_date DATE,
    expected_visitors INTEGER,
    latitude DECIMAL(10, 8),
    longitude DECIMAL(11, 8),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- çµ±åˆäº¤é€šãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS integrated_transport (
    id SERIAL PRIMARY KEY,
    region_code VARCHAR(2) REFERENCES regions(region_code),
    route_name VARCHAR(200),
    transport_type VARCHAR(50),
    operator VARCHAR(100),
    stop_count INTEGER,
    route_length_km DECIMAL(10, 2),
    geom GEOMETRY(LineString, 4326),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
CREATE INDEX idx_population_region ON integrated_population(region_code);
CREATE INDEX idx_tourism_region ON integrated_tourism(region_code);
CREATE INDEX idx_events_region ON integrated_events(region_code);
CREATE INDEX idx_transport_region ON integrated_transport(region_code);

-- ç©ºé–“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_tourism_geom ON integrated_tourism USING GIST (ST_MakePoint(longitude, latitude));
CREATE INDEX idx_events_geom ON integrated_events USING GIST (ST_MakePoint(longitude, latitude));
CREATE INDEX idx_transport_geom ON integrated_transport USING GIST (geom);
""".format(timestamp=datetime.now().isoformat())
        
        # ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        schema_file = self.integrated_dir / "unified_schema.sql"
        with open(schema_file, 'w', encoding='utf-8') as f:
            f.write(schema_sql)
            
        logger.info(f"çµ±ä¸€ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆ: {schema_file}")
        
        return schema_file
        
    def collect_all_regions(self):
        """å…¨6åœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        all_results = {
            "regions": {},
            "common": []
        }
        
        # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_regions = sorted(
            self.regions.items(),
            key=lambda x: x[1]["priority"]
        )
        
        # ä¸¦åˆ—å®Ÿè¡Œç”¨ã®ã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            # åœ°åŸŸã”ã¨ã®ãƒ‡ãƒ¼ã‚¿åé›†ã‚’ã‚µãƒ–ãƒŸãƒƒãƒˆ
            future_to_region = {}
            
            for region_key, region_info in sorted_regions:
                future = executor.submit(
                    self.collect_region_data,
                    region_key,
                    region_info
                )
                future_to_region[future] = region_key
                
            # çµæœã‚’åé›†
            for future in concurrent.futures.as_completed(future_to_region):
                region_key = future_to_region[future]
                try:
                    results = future.result()
                    all_results["regions"][region_key] = results
                except Exception as e:
                    logger.error(f"åœ°åŸŸãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼ ({region_key}): {e}")
                    all_results["regions"][region_key] = [{
                        "status": "error",
                        "region": region_key,
                        "error": str(e)
                    }]
                    
        # å…±é€šãƒ‡ãƒ¼ã‚¿åé›†
        all_results["common"] = self.collect_common_data()
        
        return all_results
        
    def generate_final_report(self, integration_summary: Dict):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_lines = [
            "="*80,
            "6åœ°åŸŸçµ±åˆãƒ‡ãƒ¼ã‚¿åé›† æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ",
            "="*80,
            f"å®Ÿè¡Œæ—¥æ™‚: {integration_summary['timestamp']}",
            "",
            "ã€åé›†çµ±è¨ˆã€‘",
            f"- å¯¾è±¡åœ°åŸŸæ•°: {integration_summary['statistics']['total_regions']}",
            f"- æˆåŠŸã—ãŸåé›†: {integration_summary['statistics']['successful_collections']}",
            f"- å¤±æ•—ã—ãŸåé›†: {integration_summary['statistics']['failed_collections']}",
            "",
            "ã€åœ°åŸŸåˆ¥çµæœã€‘"
        ]
        
        for region_key, region_data in integration_summary["regions"].items():
            report_lines.append(f"\nâ–  {region_data['name']} (ã‚³ãƒ¼ãƒ‰: {region_data['code']})")
            for collection in region_data["collections"]:
                status_mark = "âœ…" if collection["status"] == "success" else "âŒ"
                report_lines.append(f"  {status_mark} {collection['type']}: {collection['status']}")
                if collection.get("error"):
                    report_lines.append(f"     ã‚¨ãƒ©ãƒ¼: {collection['error'][:100]}...")
                    
        report_lines.extend([
            "",
            "ã€å…±é€šãƒ‡ãƒ¼ã‚¿åé›†çµæœã€‘"
        ])
        
        for data_type, result in integration_summary["common_data"].items():
            status_mark = "âœ…" if result["status"] == "success" else "âŒ"
            report_lines.append(f"  {status_mark} {data_type}: {result['status']}")
            if result.get("error"):
                report_lines.append(f"     ã‚¨ãƒ©ãƒ¼: {result['error'][:100]}...")
                
        report_lines.extend([
            "",
            "ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘",
            "1. åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼",
            "2. PostgreSQLã¸ã®ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
            "3. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¨ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ",
            "4. ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ åé›†",
            "",
            "="*80
        ])
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = self.integrated_dir / f"final_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
            
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print('\n'.join(report_lines))
        
        return report_file


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\nğŸš€ 6åœ°åŸŸçµ±åˆãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
    print("="*80)
    
    collector = SixRegionsIntegratedCollector()
    
    try:
        # 1. å…¨åœ°åŸŸãƒ‡ãƒ¼ã‚¿åé›†
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        all_results = collector.collect_all_regions()
        
        # 2. ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        logger.info("\nã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†...")
        integration_summary = collector.integrate_collected_data(all_results)
        
        # 3. çµ±ä¸€ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆ
        logger.info("\nã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆ...")
        schema_file = collector.generate_unified_database_schema()
        
        # 4. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        logger.info("\nã‚¹ãƒ†ãƒƒãƒ—4: æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ...")
        report_file = collector.generate_final_report(integration_summary)
        
        print(f"\nâœ… å‡¦ç†å®Œäº†ï¼")
        print(f"çµ±åˆãƒ‡ãƒ¼ã‚¿: {collector.integrated_dir}")
        print(f"ã‚¹ã‚­ãƒ¼ãƒ: {schema_file}")
        print(f"ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
        
    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()