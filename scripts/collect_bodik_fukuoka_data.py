#!/usr/bin/env python3
"""
BODIKï¼ˆä¹å·ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿æ¨é€²ä¼šè­°ï¼‰ç¦å²¡çœŒãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
BODIKã¯CKAN APIã‚’æä¾›ã—ã¦ãŠã‚Šã€ç¦å²¡çœŒã‚’å«ã‚€ä¹å·å„çœŒã®ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½
"""
import requests
import json
from datetime import datetime
from pathlib import Path
import logging
import time
import os

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BODIKFukuokaDataCollector:
    """BODIKçµŒç”±ã§ç¦å²¡çœŒãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    
    def __init__(self):
        self.base_url = "https://data.bodik.jp/api/3/action"
        self.base_dir = Path(__file__).parent.parent
        self.data_dir = self.base_dir / "uesugi-engine-data" / "fukuoka"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.tourism_dir = self.data_dir / "tourism"
        self.population_dir = self.data_dir / "population"
        self.transport_dir = self.data_dir / "transport"
        self.policy_dir = self.data_dir / "policy"
        
        for dir in [self.tourism_dir, self.population_dir, self.transport_dir, self.policy_dir]:
            dir.mkdir(exist_ok=True)
            
    def search_packages(self, query, organization=None, rows=100):
        """ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æ¤œç´¢"""
        url = f"{self.base_url}/package_search"
        params = {
            "q": query,
            "rows": rows,
            "start": 0
        }
        
        # ç¦å²¡çœŒã«é™å®š
        if organization:
            params["fq"] = f"organization:{organization}"
        else:
            # ç¦å²¡é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§çµã‚Šè¾¼ã¿
            params["q"] = f"{query} AND (ç¦å²¡ OR fukuoka)"
            
        all_results = []
        
        try:
            while True:
                response = requests.get(url, params=params)
                response.raise_for_status()
                data = response.json()
                
                if data["success"]:
                    results = data["result"]["results"]
                    all_results.extend(results)
                    
                    # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ç¢ºèª
                    if len(results) < rows:
                        break
                    params["start"] += rows
                    
                else:
                    logger.error(f"æ¤œç´¢å¤±æ•—: {data.get('error', 'Unknown error')}")
                    break
                    
        except Exception as e:
            logger.error(f"API ã‚¨ãƒ©ãƒ¼: {e}")
            
        return all_results
        
    def get_organizations(self):
        """çµ„ç¹”ä¸€è¦§ã‚’å–å¾—ï¼ˆç¦å²¡çœŒé–¢é€£ã‚’ç‰¹å®šï¼‰"""
        url = f"{self.base_url}/organization_list"
        
        try:
            response = requests.get(url, params={"all_fields": True})
            response.raise_for_status()
            data = response.json()
            
            if data["success"]:
                fukuoka_orgs = []
                for org in data["result"]:
                    if any(keyword in org.get("title", "").lower() or keyword in org.get("name", "").lower() 
                          for keyword in ["ç¦å²¡", "fukuoka", "åŒ—ä¹å·", "kitakyushu"]):
                        fukuoka_orgs.append({
                            "name": org["name"],
                            "title": org["title"],
                            "package_count": org.get("package_count", 0)
                        })
                        
                return fukuoka_orgs
                
        except Exception as e:
            logger.error(f"çµ„ç¹”ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
        return []
        
    def download_resource(self, resource, category_dir):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            url = resource["url"]
            format = resource.get("format", "").upper()
            
            if format not in ["CSV", "JSON", "XLS", "XLSX"]:
                return None
                
            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            filename = resource.get("name", "").replace("/", "_").replace("\\", "_")
            if not filename:
                filename = f"resource_{resource.get('id', 'unknown')}"
            
            if not filename.endswith(f".{format.lower()}"):
                filename += f".{format.lower()}"
                
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            file_path = category_dir / filename
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        
            logger.info(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {filename}")
            return file_path
            
        except Exception as e:
            logger.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
            
    def collect_tourism_data(self):
        """è¦³å…‰é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("\nã€è¦³å…‰ãƒ‡ãƒ¼ã‚¿åé›†ã€‘")
        
        keywords = ["è¦³å…‰", "å®¿æ³Š", "ã‚¤ãƒ™ãƒ³ãƒˆ", "è¦³å…‰å®¢", "ãƒ›ãƒ†ãƒ«"]
        datasets = []
        downloaded = 0
        
        for keyword in keywords:
            logger.info(f"æ¤œç´¢ä¸­: {keyword}")
            results = self.search_packages(keyword)
            
            for package in results:
                # ç¦å²¡çœŒé–¢é€£ã‹ç¢ºèª
                title = package.get("title", "")
                notes = package.get("notes", "")
                
                if not any(word in title + notes for word in ["ç¦å²¡", "åŒ—ä¹å·", "ä¹…ç•™ç±³", "å¤§ç‰Ÿç”°"]):
                    continue
                    
                dataset = {
                    "id": package["id"],
                    "title": title,
                    "organization": package.get("organization", {}).get("title", ""),
                    "resources": []
                }
                
                # ãƒªã‚½ãƒ¼ã‚¹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                for resource in package.get("resources", []):
                    if resource.get("format", "").upper() in ["CSV", "JSON", "XLS", "XLSX"]:
                        file_path = self.download_resource(resource, self.tourism_dir)
                        if file_path:
                            dataset["resources"].append({
                                "name": resource.get("name", ""),
                                "file_path": str(file_path)
                            })
                            downloaded += 1
                            time.sleep(0.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                            
                if dataset["resources"]:
                    datasets.append(dataset)
                    
        logger.info(f"è¦³å…‰ãƒ‡ãƒ¼ã‚¿: {len(datasets)}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ, {downloaded}ãƒ•ã‚¡ã‚¤ãƒ«")
        return datasets
        
    def collect_transport_data(self):
        """äº¤é€šé–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("\nã€äº¤é€šãƒ‡ãƒ¼ã‚¿åé›†ã€‘")
        
        keywords = ["äº¤é€š", "ãƒã‚¹", "é‰„é“", "åœ°ä¸‹é‰„", "GTFS", "æ™‚åˆ»è¡¨"]
        datasets = []
        downloaded = 0
        
        for keyword in keywords:
            logger.info(f"æ¤œç´¢ä¸­: {keyword}")
            results = self.search_packages(keyword)
            
            for package in results[:20]:  # æœ€å¤§20ä»¶
                dataset = {
                    "id": package["id"],
                    "title": package.get("title", ""),
                    "resources": []
                }
                
                for resource in package.get("resources", []):
                    if resource.get("format", "").upper() in ["CSV", "JSON", "GTFS"]:
                        file_path = self.download_resource(resource, self.transport_dir)
                        if file_path:
                            dataset["resources"].append({
                                "name": resource.get("name", ""),
                                "file_path": str(file_path)
                            })
                            downloaded += 1
                            time.sleep(0.5)
                            
                if dataset["resources"]:
                    datasets.append(dataset)
                    
        logger.info(f"äº¤é€šãƒ‡ãƒ¼ã‚¿: {len(datasets)}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ, {downloaded}ãƒ•ã‚¡ã‚¤ãƒ«")
        return datasets
        
    def save_collection_summary(self, all_data):
        """åé›†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        summary = {
            "timestamp": datetime.now().isoformat(),
            "source": "BODIK",
            "prefecture": "ç¦å²¡çœŒ",
            "data": all_data,
            "stats": {
                "tourism": len(all_data.get("tourism", [])),
                "transport": len(all_data.get("transport", [])),
                "total_files": sum(
                    len(ds.get("resources", [])) 
                    for category in all_data.values() 
                    for ds in category
                )
            }
        }
        
        # JSONä¿å­˜
        json_path = self.data_dir / f"bodik_collection_summary_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
            
        # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report_path = self.data_dir / f"bodik_collection_report_{timestamp}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# BODIKç¦å²¡çœŒãƒ‡ãƒ¼ã‚¿åé›†ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}\n")
            f.write(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: BODIK (https://data.bodik.jp/)\n\n")
            
            f.write("## åé›†çµ±è¨ˆ\n\n")
            f.write(f"- è¦³å…‰ãƒ‡ãƒ¼ã‚¿: {summary['stats']['tourism']}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n")
            f.write(f"- äº¤é€šãƒ‡ãƒ¼ã‚¿: {summary['stats']['transport']}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n")
            f.write(f"- ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {summary['stats']['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«\n\n")
            
            # ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            if all_data.get("tourism"):
                f.write("## è¦³å…‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n\n")
                for ds in all_data["tourism"][:5]:
                    f.write(f"- {ds['title']} ({len(ds['resources'])}ãƒ•ã‚¡ã‚¤ãƒ«)\n")
                    
            if all_data.get("transport"):
                f.write("\n## äº¤é€šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n\n")
                for ds in all_data["transport"][:5]:
                    f.write(f"- {ds['title']} ({len(ds['resources'])}ãƒ•ã‚¡ã‚¤ãƒ«)\n")
                    
        logger.info(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜:")
        logger.info(f"- JSON: {json_path}")
        logger.info(f"- Markdown: {report_path}")
        
        return json_path, report_path
        
    def run_collection(self):
        """ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å®Ÿè¡Œ"""
        logger.info("="*60)
        logger.info("BODIKç¦å²¡çœŒãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        logger.info("="*60)
        
        # ç¦å²¡çœŒé–¢é€£çµ„ç¹”ã‚’ç¢ºèª
        logger.info("\nç¦å²¡çœŒé–¢é€£çµ„ç¹”ã‚’æ¤œç´¢ä¸­...")
        fukuoka_orgs = self.get_organizations()
        if fukuoka_orgs:
            logger.info(f"ç™ºè¦‹ã—ãŸç¦å²¡çœŒé–¢é€£çµ„ç¹”: {len(fukuoka_orgs)}ä»¶")
            for org in fukuoka_orgs[:5]:
                logger.info(f"- {org['title']} ({org['package_count']}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)")
                
        # ãƒ‡ãƒ¼ã‚¿åé›†
        all_data = {
            "tourism": self.collect_tourism_data(),
            "transport": self.collect_transport_data()
        }
        
        # ã‚µãƒãƒªãƒ¼ä¿å­˜
        self.save_collection_summary(all_data)
        
        return all_data


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\nğŸ›ï¸ BODIKç¦å²¡çœŒãƒ‡ãƒ¼ã‚¿åé›†")
    print("="*60)
    print("ä¹å·ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿æ¨é€²ä¼šè­°ã®BODIKã‹ã‚‰ç¦å²¡çœŒãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¾ã™")
    print()
    
    collector = BODIKFukuokaDataCollector()
    data = collector.run_collection()
    
    print("\nâœ… åé›†å®Œäº†!")
    print(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {collector.data_dir}")
    
    # çµ±è¨ˆè¡¨ç¤º
    total_datasets = sum(len(v) for v in data.values())
    total_files = sum(
        len(ds.get("resources", [])) 
        for datasets in data.values() 
        for ds in datasets
    )
    
    print(f"\nğŸ“Š åé›†çµ±è¨ˆ:")
    print(f"- ç·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {total_datasets}")
    print(f"- ç·ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")


if __name__ == "__main__":
    main()