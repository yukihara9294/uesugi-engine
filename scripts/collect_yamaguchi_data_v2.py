#!/usr/bin/env python3
"""
å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ« ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ V2
å®Ÿéš›ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€PostgreSQLã¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæº–å‚™ã‚‚è¡Œã†
"""
import requests
import json
import csv
from datetime import datetime
from pathlib import Path
import logging
import time
from urllib.parse import urlparse
import os

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class YamaguchiDataCollectorV2:
    """å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç‰ˆ"""
    
    def __init__(self):
        self.base_url = "https://yamaguchi-opendata.jp/ckan/api/3/action"
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹
        self.base_dir = Path(__file__).parent.parent
        self.data_dir = self.base_dir / "uesugi-engine-data" / "yamaguchi"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.tourism_dir = self.data_dir / "tourism"
        self.population_dir = self.data_dir / "population"
        self.events_dir = self.data_dir / "events"
        self.policy_dir = self.data_dir / "policy"
        
        for dir in [self.tourism_dir, self.population_dir, self.events_dir, self.policy_dir]:
            dir.mkdir(exist_ok=True)
        
    def search_packages(self, query, rows=1000):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¤œç´¢ï¼ˆæœ€å¤§ä»¶æ•°å–å¾—ï¼‰"""
        url = f"{self.base_url}/package_search"
        params = {
            "q": query,
            "rows": rows
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            if data["success"]:
                return data["result"]["results"]
            else:
                return []
                
        except Exception as e:
            logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
            
    def download_resource(self, resource, category_dir):
        """ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            url = resource["url"]
            format = resource["format"].upper()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆURLã‹ã‚‰å–å¾—ã¾ãŸã¯ãƒªã‚½ãƒ¼ã‚¹åã‹ã‚‰ï¼‰
            if url.endswith(('.csv', '.json', '.xls', '.xlsx')):
                filename = os.path.basename(urlparse(url).path)
            else:
                filename = f"{resource['name']}.{format.lower()}"
            
            # æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«åã®å‡¦ç†
            filename = filename.replace('/', '_').replace('\\', '_')
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            file_path = category_dir / filename
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        
            logger.info(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {filename}")
            
            # CSVã®å ´åˆã¯æ–‡å­—ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚’è©¦ã¿ã‚‹
            if format == "CSV":
                self.convert_csv_encoding(file_path)
                
            return file_path
            
        except Exception as e:
            logger.error(f"âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ ({resource['name']}): {e}")
            return None
            
    def convert_csv_encoding(self, file_path):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’UTF-8ã«å¤‰æ›"""
        try:
            # ã¾ãšShift-JISã¨ã—ã¦èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã‚‹
            encodings = ['shift_jis', 'cp932', 'utf-8', 'utf-8-sig']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    
                    # UTF-8ã§æ›¸ãç›´ã—
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    
                    logger.info(f"æ–‡å­—ã‚³ãƒ¼ãƒ‰å¤‰æ›å®Œäº†: {encoding} â†’ UTF-8")
                    break
                except UnicodeDecodeError:
                    continue
                    
        except Exception as e:
            logger.warning(f"æ–‡å­—ã‚³ãƒ¼ãƒ‰å¤‰æ›ã‚¹ã‚­ãƒƒãƒ—: {e}")
            
    def collect_all_data(self):
        """å…¨ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        logger.info("="*60)
        logger.info("å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        logger.info("="*60)
        
        # åé›†çµ±è¨ˆ
        stats = {
            "tourism": {"count": 0, "downloaded": 0},
            "population": {"count": 0, "downloaded": 0},
            "events": {"count": 0, "downloaded": 0},
            "policy": {"count": 0, "downloaded": 0}
        }
        
        # 1. è¦³å…‰ãƒ‡ãƒ¼ã‚¿
        logger.info("\nã€è¦³å…‰é–¢é€£ãƒ‡ãƒ¼ã‚¿åé›†ã€‘")
        tourism_keywords = ["è¦³å…‰", "å®¿æ³Š", "è¦³å…‰å®¢", "æ¥è¨ªè€…", "è¦³å…‰æ–½è¨­"]
        tourism_datasets = self._collect_by_keywords(tourism_keywords, self.tourism_dir, stats["tourism"])
        
        # 2. äººå£ãƒ‡ãƒ¼ã‚¿
        logger.info("\nã€äººå£çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†ã€‘")
        population_keywords = ["äººå£", "ä¸–å¸¯", "å¹´é½¢åˆ¥", "åœ°åŸŸåˆ¥äººå£"]
        population_datasets = self._collect_by_keywords(population_keywords, self.population_dir, stats["population"])
        
        # 3. ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
        logger.info("\nã€ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±åé›†ã€‘")
        event_keywords = ["ã‚¤ãƒ™ãƒ³ãƒˆ", "ç¥­ã‚Š", "å‚¬ã—", "è¡Œäº‹", "ãƒ•ã‚§ã‚¹ãƒ†ã‚£ãƒãƒ«"]
        event_datasets = self._collect_by_keywords(event_keywords, self.events_dir, stats["events"])
        
        # 4. æ”¿ç­–é–¢é€£ãƒ‡ãƒ¼ã‚¿
        logger.info("\nã€æ”¿ç­–ãƒ»è¡Œæ”¿ãƒ‡ãƒ¼ã‚¿åé›†ã€‘")
        policy_keywords = ["äºˆç®—", "æ±ºç®—", "è¨ˆç”»", "æ–½ç­–", "äº‹æ¥­", "çµ±è¨ˆ"]
        policy_datasets = self._collect_by_keywords(policy_keywords, self.policy_dir, stats["policy"])
        
        # ã‚µãƒãƒªãƒ¼ä½œæˆ
        self._save_summary(stats, {
            "tourism": tourism_datasets,
            "population": population_datasets,
            "events": event_datasets,
            "policy": policy_datasets
        })
        
        return stats
        
    def _collect_by_keywords(self, keywords, category_dir, stats):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        collected_ids = set()
        datasets = []
        
        for keyword in keywords:
            logger.info(f"'{keyword}'ã§æ¤œç´¢ä¸­...")
            results = self.search_packages(keyword)
            
            for dataset in results:
                if dataset["id"] in collected_ids:
                    continue
                    
                collected_ids.add(dataset["id"])
                dataset_info = {
                    "id": dataset["id"],
                    "name": dataset["name"],
                    "title": dataset["title"],
                    "notes": dataset.get("notes", ""),
                    "organization": dataset.get("organization", {}).get("title", ""),
                    "resources": []
                }
                
                # CSV/Excel/JSONãƒªã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                for resource in dataset.get("resources", []):
                    if resource["format"].upper() in ["CSV", "JSON", "XLS", "XLSX"]:
                        file_path = self.download_resource(resource, category_dir)
                        if file_path:
                            dataset_info["resources"].append({
                                "name": resource["name"],
                                "format": resource["format"],
                                "file_path": str(file_path),
                                "url": resource["url"]
                            })
                            stats["downloaded"] += 1
                        time.sleep(0.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
                        
                if dataset_info["resources"]:
                    datasets.append(dataset_info)
                    stats["count"] += 1
                    
        logger.info(f"åé›†å®Œäº†: {stats['count']}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ, {stats['downloaded']}ãƒ•ã‚¡ã‚¤ãƒ«")
        return datasets
        
    def _save_summary(self, stats, all_data):
        """åé›†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # çµ±è¨ˆæƒ…å ±
        summary = {
            "timestamp": datetime.now().isoformat(),
            "stats": stats,
            "total_datasets": sum(s["count"] for s in stats.values()),
            "total_files": sum(s["downloaded"] for s in stats.values()),
            "data": all_data
        }
        
        # JSONä¿å­˜
        json_path = self.data_dir / f"collection_summary_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
            
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
        report_path = self.data_dir / f"collection_report_{timestamp}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("="*60 + "\n")
            f.write(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now()}\n")
            f.write(f"ç·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {summary['total_datasets']}ä»¶\n")
            f.write(f"ç·ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {summary['total_files']}ä»¶\n\n")
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
            for category, stat in stats.items():
                f.write(f"\nã€{category.upper()}ã€‘\n")
                f.write(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {stat['count']}ä»¶\n")
                f.write(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°: {stat['downloaded']}ãƒ•ã‚¡ã‚¤ãƒ«\n")
                
                # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
                if category in all_data and all_data[category]:
                    f.write("ä¸»ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:\n")
                    for ds in all_data[category][:5]:
                        f.write(f"- {ds['title']} ({len(ds['resources'])}ãƒ•ã‚¡ã‚¤ãƒ«)\n")
                        
        # PostgreSQLç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
        self._generate_import_script(all_data)
        
        logger.info(f"\nâœ… åé›†å®Œäº†ï¼")
        logger.info(f"ã‚µãƒãƒªãƒ¼: {json_path}")
        logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        
        return json_path, report_path
        
    def _generate_import_script(self, all_data):
        """PostgreSQLã‚¤ãƒ³ãƒãƒ¼ãƒˆç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ"""
        script_path = self.data_dir / "import_to_postgresql.sh"
        
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write("#!/bin/bash\n")
            f.write("# å±±å£çœŒãƒ‡ãƒ¼ã‚¿PostgreSQLã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ\n\n")
            
            f.write("# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæƒ…å ±\n")
            f.write("DB_HOST=${DB_HOST:-localhost}\n")
            f.write("DB_PORT=${DB_PORT:-5432}\n")
            f.write("DB_NAME=${DB_NAME:-uesugi_db}\n")
            f.write("DB_USER=${DB_USER:-postgres}\n\n")
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            for category, datasets in all_data.items():
                f.write(f"\n# {category.upper()} ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n")
                
                for dataset in datasets:
                    for resource in dataset["resources"]:
                        if resource["format"].upper() == "CSV":
                            table_name = f"yamaguchi_{category}_{dataset['name'].replace('-', '_')}"
                            file_path = resource["file_path"]
                            
                            f.write(f"\n# {dataset['title']}\n")
                            f.write(f"echo 'Importing {dataset['title']}...'\n")
                            f.write(f"psql -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME -c \"\n")
                            f.write(f"  DROP TABLE IF EXISTS {table_name};\n")
                            f.write(f"  CREATE TABLE {table_name} AS\n")
                            f.write(f"  SELECT * FROM CSVREAD('{file_path}');\n")
                            f.write(f"\"\n")
                            
        os.chmod(script_path, 0o755)
        logger.info(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ: {script_path}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\nğŸ›ï¸ å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ V2")
    print("="*60)
    
    collector = YamaguchiDataCollectorV2()
    
    # å…¨ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
    stats = collector.collect_all_data()
    
    print("\nâœ… åé›†å®Œäº†çµ±è¨ˆ:")
    print(f"- ç·ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {sum(s['count'] for s in stats.values())}ä»¶")
    print(f"- ç·ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {sum(s['downloaded'] for s in stats.values())}ä»¶")
    print(f"\nãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {collector.data_dir}")
    
    # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ‰‹é †è¡¨ç¤º
    print("\nğŸ“ PostgreSQLã¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ:")
    print("1. Dockerç’°å¢ƒãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª")
    print("2. cd uesugi-engine-data/yamaguchi")
    print("3. ./import_to_postgresql.sh ã‚’å®Ÿè¡Œ")


if __name__ == "__main__":
    main()