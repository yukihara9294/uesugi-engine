#!/usr/bin/env python3
"""
APIã‚­ãƒ¼ä¸è¦ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
èªè¨¼ãªã—ã§å–å¾—å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–‹å§‹
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'backend', 'app'))

import asyncio
import logging
from datetime import datetime
from pathlib import Path
import json
import requests
from typing import Dict, List

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
log_dir = Path("uesugi-engine-data/logs")
log_dir.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / f'collection_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class FreeOpenDataCollector:
    """APIã‚­ãƒ¼ä¸è¦ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†"""
    
    def __init__(self):
        self.data_dir = Path("uesugi-engine-data")
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Uesugi-Engine/1.0 (https://github.com/yukihara9294/uesugi-engine)'
        })
        
    def collect_all(self) -> Dict:
        """å…¨ã¦ã®ç„¡æ–™ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        results = {
            "timestamp": datetime.now().isoformat(),
            "weather": self.collect_weather_data(),
            "earthquakes": self.collect_earthquake_data(),
            "prefectures": self.collect_prefecture_data(),
            "gtfs": self.collect_gtfs_data(),
            "statistics": self.collect_basic_statistics(),
            "environmental": self.collect_environmental_data(),
            "municipal": self.collect_municipal_opendata(),
            "event_calendars": self.collect_event_calendars(),
            "air_quality": self.collect_air_quality_data(),
            "tourism": self.collect_tourism_data(),
            "government": self.collect_government_opendata()
        }
        
        # çµæœã‚’ä¿å­˜
        self._save_results(results)
        return results
        
    def collect_weather_data(self) -> Dict:
        """æ°—è±¡ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆOpen-Meteoï¼‰"""
        logger.info("=== æ°—è±¡ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        weather_data = {}
        
        # ä¸»è¦éƒ½å¸‚ã®åº§æ¨™
        cities = {
            "åºƒå³¶å¸‚": {"lat": 34.3853, "lng": 132.4553},
            "å±±å£å¸‚": {"lat": 34.1859, "lng": 131.4705},
            "ç¦å²¡å¸‚": {"lat": 33.5904, "lng": 130.4017},
            "å¤§é˜ªå¸‚": {"lat": 34.6937, "lng": 135.5023},
            "æ±äº¬éƒ½": {"lat": 35.6762, "lng": 139.6503}
        }
        
        for city_name, coords in cities.items():
            try:
                url = "https://api.open-meteo.com/v1/forecast"
                params = {
                    "latitude": coords["lat"],
                    "longitude": coords["lng"],
                    "current": "temperature_2m,relative_humidity_2m,precipitation,weather_code,wind_speed_10m",
                    "hourly": "temperature_2m,precipitation_probability,weather_code",
                    "daily": "weather_code,temperature_2m_max,temperature_2m_min,precipitation_sum",
                    "timezone": "Asia/Tokyo",
                    "forecast_days": 7
                }
                
                response = self.session.get(url, params=params, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                weather_data[city_name] = {
                    "status": "success",
                    "current": data.get("current", {}),
                    "daily_forecast": data.get("daily", {})
                }
                logger.info(f"âœ“ {city_name}ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"âœ— {city_name}ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {e}")
                weather_data[city_name] = {"status": "error", "error": str(e)}
                
        return weather_data
        
    def collect_earthquake_data(self) -> List[Dict]:
        """åœ°éœ‡ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆæ°—è±¡åºï¼‰"""
        logger.info("=== åœ°éœ‡ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        
        try:
            url = "https://www.jma.go.jp/bosai/quake/data/list.json"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            earthquakes = response.json()
            
            # æœ€æ–°100ä»¶ã‚’ä¿å­˜
            recent_quakes = earthquakes[:100] if len(earthquakes) > 100 else earthquakes
            
            logger.info(f"âœ“ åœ°éœ‡ãƒ‡ãƒ¼ã‚¿ {len(recent_quakes)}ä»¶å–å¾—æˆåŠŸ")
            return recent_quakes
            
        except Exception as e:
            logger.error(f"âœ— åœ°éœ‡ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {e}")
            return []
            
    def collect_prefecture_data(self) -> Dict:
        """éƒ½åºœçœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰åé›†"""
        logger.info("=== éƒ½åºœçœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        prefecture_data = {}
        
        # å„éƒ½åºœçœŒã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ï¼ˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãƒªãƒ³ã‚¯ï¼‰
        open_data_urls = {
            "æ±äº¬éƒ½": {
                "portal_info": {
                    "url": "https://portal.data.metro.tokyo.lg.jp/",
                    "description": "æ±äº¬éƒ½ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ã‚µã‚¤ãƒˆ"
                },
                "data_catalog": {
                    "url": "https://catalog.data.metro.tokyo.lg.jp/dataset",
                    "description": "æ±äº¬éƒ½ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°"
                }
            },
            "å¤§é˜ªåºœ": {
                "portal_info": {
                    "url": "https://www.city.osaka.lg.jp/contents/wdu290/opendata/",
                    "description": "å¤§é˜ªå¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆ"
                },
                "pref_portal": {
                    "url": "https://www.pref.osaka.lg.jp/it-suishin/smart/opendata.html",
                    "description": "å¤§é˜ªåºœã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿"
                }
            },
            "åºƒå³¶çœŒ": {
                "portal_info": {
                    "url": "https://www.pref.hiroshima.lg.jp/soshiki/266/opendata.html",
                    "description": "åºƒå³¶çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"
                },
                "city_portal": {
                    "url": "https://www.city.hiroshima.lg.jp/site/opendata/",
                    "description": "åºƒå³¶å¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿"
                }
            },
            "å±±å£çœŒ": {
                "portal_info": {
                    "url": "https://yamaguchi-opendata.jp/",
                    "description": "å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ã‚µã‚¤ãƒˆ"
                },
                "tourism": {
                    "url": "https://yamaguchi-opendata.jp/ckan/dataset/350001-tourism",
                    "description": "è¦³å…‰é–¢é€£ãƒ‡ãƒ¼ã‚¿"
                },
                "facilities": {
                    "url": "https://yamaguchi-opendata.jp/ckan/dataset/public-facilities",
                    "description": "å…¬å…±æ–½è¨­ãƒ‡ãƒ¼ã‚¿"
                }
            },
            "ç¦å²¡çœŒ": {
                "portal_info": {
                    "url": "https://www.open-governmentdata.org/fukuoka-city/",
                    "description": "ç¦å²¡å¸‚ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿"
                },
                "facilities": {
                    "url": "https://www.open-governmentdata.org/fukuoka-city/dataset/facility-info",
                    "description": "æ–½è¨­æƒ…å ±"
                },
                "evacuation": {
                    "url": "https://www.open-governmentdata.org/fukuoka-city/dataset/evacuation-center",
                    "description": "é¿é›£æ‰€æƒ…å ±"
                }
            }
        }
        
        for prefecture, datasets in open_data_urls.items():
            prefecture_data[prefecture] = {}
            
            for dataset_name, dataset_info in datasets.items():
                if isinstance(dataset_info, dict):
                    url = dataset_info.get("url", "")
                    description = dataset_info.get("description", dataset_name)
                else:
                    url = dataset_info
                    description = dataset_name
                    
                try:
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿å–å¾—ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª
                    head_response = self.session.head(url, timeout=5)
                    file_size = int(head_response.headers.get('content-length', 0))
                    
                    # 10MBä»¥ä¸‹ãªã‚‰å–å¾—
                    if file_size < 10 * 1024 * 1024:
                        response = self.session.get(url, timeout=30)
                        response.raise_for_status()
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                        filename = f"{prefecture}_{dataset_name}_{datetime.now().strftime('%Y%m%d')}.csv"
                        filepath = self.data_dir / "raw" / "prefectures" / filename
                        filepath.parent.mkdir(parents=True, exist_ok=True)
                        
                        with open(filepath, 'wb') as f:
                            f.write(response.content)
                            
                        prefecture_data[prefecture][dataset_name] = {
                            "status": "success",
                            "description": description,
                            "file": str(filepath),
                            "size": file_size
                        }
                        logger.info(f"âœ“ {prefecture} - {dataset_name} å–å¾—æˆåŠŸ")
                    else:
                        prefecture_data[prefecture][dataset_name] = {
                            "status": "skipped",
                            "reason": "file_too_large",
                            "size": file_size
                        }
                        
                except Exception as e:
                    logger.error(f"âœ— {prefecture} - {dataset_name} å–å¾—å¤±æ•—: {e}")
                    prefecture_data[prefecture][dataset_name] = {
                        "status": "error",
                        "error": str(e)
                    }
                    
        return prefecture_data
        
    def collect_gtfs_data(self) -> Dict:
        """GTFSï¼ˆå…¬å…±äº¤é€šï¼‰ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== GTFSãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        gtfs_data = {}
        
        # ç¢ºå®Ÿã«åˆ©ç”¨å¯èƒ½ãªGTFSãƒ‡ãƒ¼ã‚¿
        gtfs_feeds = {
            "ODPT": {
                "url": "https://www.odpt.org/en/overview/",
                "type": "å…¬å…±äº¤é€šã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼",
                "note": "ç™»éŒ²å¿…è¦"
            },
            "åå‹ãƒã‚¹": {
                "url": "https://www.tokachibus.jp/rosenbus/opendata/",
                "type": "ãƒã‚¹ï¼ˆGTFS-JPï¼‰"
            },
            "æ±äº¬GTFS": {
                "url": "https://github.com/MKuranowski/TokyoGTFS",
                "type": "æ±äº¬åœé‰„é“ãƒ»ãƒã‚¹",
                "note": "GitHubãƒªãƒã‚¸ãƒˆãƒª"
            },
            "TransitFeeds": {
                "url": "https://transitfeeds.com/location/asia/japan",
                "type": "æ—¥æœ¬ã®GTFSãƒ•ã‚£ãƒ¼ãƒ‰é›†ç´„",
                "note": "2025å¹´12æœˆå»ƒæ­¢äºˆå®š"
            }
        }
        
        for operator, info in gtfs_feeds.items():
            try:
                # URLã®å­˜åœ¨ç¢ºèª
                response = self.session.head(info["url"], timeout=5)
                if response.status_code == 200:
                    gtfs_data[operator] = {
                        "status": "available",
                        "url": info["url"],
                        "type": info["type"]
                    }
                    logger.info(f"âœ“ {operator} GTFSåˆ©ç”¨å¯èƒ½")
                else:
                    gtfs_data[operator] = {
                        "status": "not_found",
                        "code": response.status_code
                    }
                    
            except Exception as e:
                logger.error(f"âœ— {operator} GTFSç¢ºèªå¤±æ•—: {e}")
                gtfs_data[operator] = {
                    "status": "error",
                    "error": str(e)
                }
                
        return gtfs_data
        
    def collect_basic_statistics(self) -> Dict:
        """åŸºæœ¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== åŸºæœ¬çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        
        # e-Statã®çµ±è¨ˆè¡¨ä¸€è¦§ï¼ˆAPIã‚­ãƒ¼ãªã—ã§ã‚‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯å–å¾—å¯èƒ½ï¼‰
        stats = {
            "population": {
                "stat_id": "0003412316",
                "name": "äººå£æ¨è¨ˆ"
            },
            "tourism": {
                "stat_id": "0003165838", 
                "name": "å®¿æ³Šæ—…è¡Œçµ±è¨ˆ"
            }
        }
        
        results = {}
        for stat_type, info in stats.items():
            results[stat_type] = {
                "name": info["name"],
                "stat_id": info["stat_id"],
                "note": "APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—å¯èƒ½ã€‚"
            }
            
        return results
        
    def collect_environmental_data(self) -> Dict:
        """ç’°å¢ƒãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== ç’°å¢ƒãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        env_data = {}
        
        # ç’°å¢ƒçœå¤§æ°—æ±šæŸ“ç‰©è³ªåºƒåŸŸç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãã‚‰ã¾ã‚å›ï¼‰
        env_sources = {
            "å¤§æ°—ç’°å¢ƒ": {
                "soramame": {
                    "url": "http://soramame.env.go.jp/",
                    "description": "å¤§æ°—æ±šæŸ“ç‰©è³ªåºƒåŸŸç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ",
                    "note": "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿"
                }
            },
            "æ²³å·æ°´è³ª": {
                "water_info": {
                    "url": "http://www1.river.go.jp/",
                    "description": "æ°´æ–‡æ°´è³ªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹",
                    "note": "å›½åœŸäº¤é€šçœ"
                }
            },
            "æ”¾å°„ç·š": {
                "radiation": {
                    "url": "https://radioactivity.nra.go.jp/en/",
                    "description": "æ”¾å°„ç·šãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æƒ…å ±",
                    "note": "åŸå­åŠ›è¦åˆ¶å§”å“¡ä¼š"
                }
            }
        }
        
        for category, sources in env_sources.items():
            env_data[category] = sources
            logger.info(f"ç’°å¢ƒãƒ‡ãƒ¼ã‚¿ã‚«ãƒ†ã‚´ãƒª: {category}")
            
        return env_data
        
    def collect_municipal_opendata(self) -> Dict:
        """å¸‚ç”ºæ‘ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== å¸‚ç”ºæ‘ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        
        municipal_data = {
            "åºƒå³¶çœŒ": {
                "åºƒå³¶å¸‚": {
                    "url": "https://www.city.hiroshima.lg.jp/site/opendata/",
                    "datasets": ["é¿é›£æ‰€", "AEDè¨­ç½®å ´æ‰€", "å…¬å…±æ–½è¨­"]
                },
                "å‘‰å¸‚": {
                    "url": "https://www.city.kure.lg.jp/soshiki/7/opendata.html",
                    "datasets": ["è¦³å…‰æ–½è¨­", "å…¬å…±æ–½è¨­"]
                },
                "ç¦å±±å¸‚": {
                    "url": "https://www.city.fukuyama.hiroshima.jp/soshiki/johokanri/126167.html",
                    "datasets": ["ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ä¸€è¦§"]
                }
            },
            "å±±å£çœŒ": {
                "ä¸‹é–¢å¸‚": {
                    "url": "https://www.city.shimonoseki.lg.jp/soshiki/12/4181.html",
                    "datasets": ["å…¬å…±æ–½è¨­", "è¦³å…‰æƒ…å ±"]
                },
                "å®‡éƒ¨å¸‚": {
                    "url": "https://www.city.ube.yamaguchi.jp/shisei/toukei/opendata/",
                    "datasets": ["è¦³å…‰ã‚¹ãƒãƒƒãƒˆ", "ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±"]
                }
            },
            "ç¦å²¡çœŒ": {
                "åŒ—ä¹å·å¸‚": {
                    "url": "https://www.city.kitakyushu.lg.jp/soumu/file_0308.html",
                    "datasets": ["å…¬å…±æ–½è¨­", "é¿é›£æ‰€", "è¦³å…‰æƒ…å ±"]
                },
                "ä¹…ç•™ç±³å¸‚": {
                    "url": "https://www.city.kurume.fukuoka.jp/1080shisei/2040keikaku/3090jouhou/4020opendata/",
                    "datasets": ["æ–½è¨­æƒ…å ±", "çµ±è¨ˆæƒ…å ±"]
                }
            },
            "å¤§é˜ªåºœ": {
                "å ºå¸‚": {
                    "url": "https://www.city.sakai.lg.jp/shisei/tokei/opendata/",
                    "datasets": ["é¿é›£æ‰€", "å…¬å…±æ–½è¨­", "çµ±è¨ˆ"]
                },
                "æ±å¤§é˜ªå¸‚": {
                    "url": "https://www.city.higashiosaka.lg.jp/0000027168.html",
                    "datasets": ["å…¬å…±æ–½è¨­ä¸€è¦§"]
                }
            },
            "æ±äº¬éƒ½": {
                "ä¸–ç”°è°·åŒº": {
                    "url": "https://www.city.setagaya.lg.jp/mokuji/kusei/002/006/001/d00132106.html",
                    "datasets": ["æ–½è¨­æƒ…å ±", "çµ±è¨ˆãƒ‡ãƒ¼ã‚¿"]
                },
                "æ–°å®¿åŒº": {
                    "url": "https://www.city.shinjuku.lg.jp/kusei/file09_00001.html",
                    "datasets": ["å…¬å…±æ–½è¨­", "çµ±è¨ˆæƒ…å ±"]
                }
            }
        }
        
        return municipal_data
        
    def collect_event_calendars(self) -> Dict:
        """ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        
        event_sources = {
            "è¦³å…‰ã‚¤ãƒ™ãƒ³ãƒˆ": {
                "jnto": {
                    "url": "https://www.jnto.go.jp/",
                    "description": "æ—¥æœ¬æ”¿åºœè¦³å…‰å±€ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±",
                    "note": "å¤šè¨€èªå¯¾å¿œ"
                },
                "local_events": {
                    "hiroshima": "https://www.hiroshima-navi.or.jp/",
                    "yamaguchi": "https://yamaguchi-tourism.jp/",
                    "fukuoka": "https://yokanavi.com/",
                    "osaka": "https://osaka-info.jp/",
                    "tokyo": "https://www.gotokyo.org/"
                }
            },
            "æ–‡åŒ–ã‚¤ãƒ™ãƒ³ãƒˆ": {
                "museums": {
                    "description": "åšç‰©é¤¨ãƒ»ç¾è¡“é¤¨ã‚¤ãƒ™ãƒ³ãƒˆ",
                    "note": "å„æ–½è¨­ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰åˆ©ç”¨å¯èƒ½"
                }
            },
            "ã‚¹ãƒãƒ¼ãƒ„ã‚¤ãƒ™ãƒ³ãƒˆ": {
                "sports": {
                    "description": "ã‚¹ãƒãƒ¼ãƒ„ã‚¤ãƒ™ãƒ³ãƒˆãƒ»å¤§ä¼šæƒ…å ±",
                    "note": "å„ç«¶æŠ€å›£ä½“ã‚µã‚¤ãƒˆã‹ã‚‰å–å¾—"
                }
            }
        }
        
        return event_sources
        
    def collect_air_quality_data(self) -> Dict:
        """å¤§æ°—è³ªãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== å¤§æ°—è³ªãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        
        air_quality_data = {
            "sources": {
                "soramame": {
                    "url": "http://soramame.env.go.jp/",
                    "description": "ç’°å¢ƒçœå¤§æ°—æ±šæŸ“ç‰©è³ªåºƒåŸŸç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ",
                    "note": "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿"
                },
                "pm25": {
                    "url": "http://pm25.jp/",
                    "description": "PM2.5ã¾ã¨ã‚",
                    "note": "å…¨å›½ã®PM2.5æƒ…å ±"
                }
            },
            "note": "å¤§æ°—è³ªãƒ‡ãƒ¼ã‚¿ã¯Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå¿…è¦ãªå ´åˆãŒå¤šã„"
        }
        
        return air_quality_data
        
    def collect_tourism_data(self) -> Dict:
        """è¦³å…‰çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== è¦³å…‰çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        
        tourism_data = {
            "JNTO": {
                "statistics_portal": {
                    "url": "https://statistics.jnto.go.jp/en/graph/",
                    "description": "æ—¥æœ¬æ”¿åºœè¦³å…‰å±€çµ±è¨ˆã‚µã‚¤ãƒˆ",
                    "note": "CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½"
                },
                "visitor_arrivals": {
                    "url": "https://www.jnto.go.jp/statistics/data/",
                    "description": "è¨ªæ—¥å¤–å®¢çµ±è¨ˆ",
                    "note": "æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿"
                }
            },
            "è¦³å…‰åº": {
                "statistics": {
                    "url": "https://www.mlit.go.jp/kankocho/siryou/toukei/",
                    "description": "è¦³å…‰çµ±è¨ˆãƒ»ç™½æ›¸",
                    "note": "å®¿æ³Šæ—…è¡Œçµ±è¨ˆãªã©"
                }
            },
            "åœ°åŸŸåˆ¥": {
                "hiroshima": {
                    "url": "https://www.pref.hiroshima.lg.jp/soshiki/78/",
                    "description": "åºƒå³¶çœŒè¦³å…‰çµ±è¨ˆ"
                },
                "yamaguchi": {
                    "url": "https://www.pref.yamaguchi.lg.jp/soshiki/123/",
                    "description": "å±±å£çœŒè¦³å…‰çµ±è¨ˆ"
                },
                "fukuoka": {
                    "url": "https://www.pref.fukuoka.lg.jp/contents/kanko-tokei.html",
                    "description": "ç¦å²¡çœŒè¦³å…‰çµ±è¨ˆ"
                }
            }
        }
        
        return tourism_data
        
    def collect_government_opendata(self) -> Dict:
        """æ”¿åºœã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("=== æ”¿åºœã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ ===")
        
        gov_data = {
            "DATA.GO.JP": {
                "portal": {
                    "url": "https://www.data.go.jp/",
                    "description": "æ—¥æœ¬æ”¿åºœã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«",
                    "note": "çœåºæ¨ªæ–­çš„ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°"
                }
            },
            "e-Stat": {
                "portal": {
                    "url": "https://www.e-stat.go.jp/",
                    "description": "æ”¿åºœçµ±è¨ˆã®ç·åˆçª“å£",
                    "note": "ä¸€éƒ¨ãƒ‡ãƒ¼ã‚¿ã¯APIã‚­ãƒ¼ä¸è¦"
                },
                "regional": {
                    "url": "https://www.e-stat.go.jp/regional-statistics/ssdsview",
                    "description": "åœ°åŸŸçµ±è¨ˆ",
                    "note": "å¸‚åŒºç”ºæ‘ãƒ‡ãƒ¼ã‚¿"
                }
            },
            "ç·å‹™çœçµ±è¨ˆå±€": {
                "data": {
                    "url": "https://www.stat.go.jp/data/",
                    "description": "å„ç¨®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿",
                    "note": "å›½å‹¢èª¿æŸ»ã€å®¶è¨ˆèª¿æŸ»ãªã©"
                }
            },
            "å›½åœŸäº¤é€šçœ": {
                "gis": {
                    "url": "https://nlftp.mlit.go.jp/",
                    "description": "å›½åœŸæ•°å€¤æƒ…å ±",
                    "note": "GISãƒ‡ãƒ¼ã‚¿ï¼ˆç„¡æ–™ï¼‰"
                },
                "plateau": {
                    "url": "https://www.mlit.go.jp/plateau/",
                    "description": "PLATEAUï¼ˆ3Déƒ½å¸‚ãƒ¢ãƒ‡ãƒ«ï¼‰",
                    "note": "ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿"
                }
            }
        }
        
        return gov_data
        
    def _save_results(self, results: Dict):
        """çµæœã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONå½¢å¼ã§ä¿å­˜
        output_path = self.data_dir / "collection_results" / f"free_data_{timestamp}.json"
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
            
        logger.info(f"çµæœã‚’ä¿å­˜: {output_path}")
        
        # ã‚µãƒãƒªãƒ¼ä½œæˆ
        self._create_summary(results)
        
    def _create_summary(self, results: Dict):
        """åé›†çµæœã®ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        summary = []
        summary.append("\n" + "="*60)
        summary.append("ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒãƒªãƒ¼")
        summary.append("="*60)
        summary.append(f"å®Ÿè¡Œæ™‚åˆ»: {results['timestamp']}")
        summary.append("")
        
        # æ°—è±¡ãƒ‡ãƒ¼ã‚¿
        weather_success = sum(1 for city in results['weather'].values() 
                            if city.get('status') == 'success')
        summary.append(f"æ°—è±¡ãƒ‡ãƒ¼ã‚¿: {weather_success}/{len(results['weather'])} éƒ½å¸‚æˆåŠŸ")
        
        # åœ°éœ‡ãƒ‡ãƒ¼ã‚¿
        summary.append(f"åœ°éœ‡ãƒ‡ãƒ¼ã‚¿: {len(results['earthquakes'])} ä»¶å–å¾—")
        
        # éƒ½åºœçœŒãƒ‡ãƒ¼ã‚¿
        prefecture_count = len(results.get('prefectures', {}))
        summary.append(f"éƒ½åºœçœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿: {prefecture_count} éƒ½åºœçœŒåˆ†ã®ãƒ‡ãƒ¼ã‚¿åé›†")
        
        # GTFSãƒ‡ãƒ¼ã‚¿
        gtfs_available = sum(1 for op in results.get('gtfs', {}).values() 
                           if op.get('status') == 'available')
        summary.append(f"GTFSãƒ‡ãƒ¼ã‚¿: {gtfs_available}/{len(results.get('gtfs', {}))} äº‹æ¥­è€…åˆ©ç”¨å¯èƒ½")
        
        # ç’°å¢ƒãƒ‡ãƒ¼ã‚¿
        env_categories = len(results.get('environmental', {}))
        summary.append(f"ç’°å¢ƒãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°: {env_categories} ã‚«ãƒ†ã‚´ãƒª")
        
        # å¸‚ç”ºæ‘ãƒ‡ãƒ¼ã‚¿
        municipal_prefectures = len(results.get('municipal', {}))
        municipal_cities = sum(len(cities) for cities in results.get('municipal', {}).values())
        summary.append(f"å¸‚ç”ºæ‘ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿: {municipal_prefectures} éƒ½åºœçœŒã€{municipal_cities} å¸‚åŒº")
        
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼
        event_categories = len(results.get('event_calendars', {}))
        summary.append(f"ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼: {event_categories} ã‚«ãƒ†ã‚´ãƒª")
        
        # å¤§æ°—è³ªãƒ‡ãƒ¼ã‚¿
        air_quality_sources = len(results.get('air_quality', {}).get('sources', {}))
        summary.append(f"å¤§æ°—è³ªãƒ‡ãƒ¼ã‚¿: {air_quality_sources} ã‚½ãƒ¼ã‚¹")
        
        # è¦³å…‰ãƒ‡ãƒ¼ã‚¿
        tourism_categories = len(results.get('tourism', {}))
        summary.append(f"è¦³å…‰çµ±è¨ˆãƒ‡ãƒ¼ã‚¿: {tourism_categories} ã‚«ãƒ†ã‚´ãƒª")
        
        # æ”¿åºœã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿
        gov_sources = len(results.get('government', {}))
        summary.append(f"æ”¿åºœã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿: {gov_sources} ã‚½ãƒ¼ã‚¹")
        
        summary.append("="*60)
        
        summary_text = "\n".join(summary)
        print(summary_text)
        
        # ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        summary_path = self.data_dir / "collection_results" / "latest_summary.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary_text)


async def collect_realtime_feeds():
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰ã®è¨­å®š"""
    logger.info("=== ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®š ===")
    
    feeds = {
        "weather": {
            "open_meteo": {
                "url": "https://api.open-meteo.com/v1/forecast",
                "interval": 3600,  # 1æ™‚é–“
                "description": "æ°—è±¡äºˆå ±ãƒ‡ãƒ¼ã‚¿"
            }
        },
        "earthquake": {
            "jma": {
                "url": "https://www.jma.go.jp/bosai/quake/data/list.json",
                "interval": 300,  # 5åˆ†
                "description": "åœ°éœ‡æƒ…å ±"
            }
        },
        "transport": {
            "description": "äº¤é€šæƒ…å ±ï¼ˆå„äº‹æ¥­è€…ã®APIã‚­ãƒ¼ãŒå¿…è¦ï¼‰"
        }
    }
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    config_path = Path("uesugi-engine-data/config/realtime_feeds.json")
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(feeds, f, ensure_ascii=False, indent=2)
        
    logger.info(f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šã‚’ä¿å­˜: {config_path}")
    

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ Uesugi Engine ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
    print("APIã‚­ãƒ¼ä¸è¦ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åé›†ã‚’é–‹å§‹ã—ã¾ã™...\n")
    
    # ç„¡æ–™ãƒ‡ãƒ¼ã‚¿åé›†
    collector = FreeOpenDataCollector()
    results = collector.collect_all()
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®š
    asyncio.run(collect_realtime_feeds())
    
    print("\nâœ… åé›†å®Œäº†ï¼")
    print("\nğŸ“Œ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. åé›†çµæœã‚’ç¢ºèª: uesugi-engine-data/collection_results/")
    print("2. APIã‚­ãƒ¼ãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿:")
    print("   - e-Stat (çµ±è¨ˆãƒ‡ãƒ¼ã‚¿)")
    print("   - ODPT (è©³ç´°ãªäº¤é€šãƒ‡ãƒ¼ã‚¿)")
    print("   - Twitter (SNSåˆ†æ)")
    print("   - å„è‡ªæ²»ä½“ã®API")
    print("\nğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ç™»éŒ²ãŒå¿…è¦ãªã‚µãƒ¼ãƒ“ã‚¹ãŒã‚ã‚Œã°ã€ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()