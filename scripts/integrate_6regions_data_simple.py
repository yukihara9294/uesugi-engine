#!/usr/bin/env python3
"""
6åœ°åŸŸãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¾å­˜ãªã—ãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""
import json
import csv
from pathlib import Path
import logging
from datetime import datetime
import os

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SixRegionsDataIntegratorSimple:
    """6åœ°åŸŸãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¯ãƒ©ã‚¹ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent.parent
        self.data_dir = self.base_dir / "uesugi-engine-data"
        self.integrated_dir = self.data_dir / "6regions_integrated"
        self.integrated_dir.mkdir(parents=True, exist_ok=True)
        
        # åœ°åŸŸæƒ…å ±
        self.regions = {
            "hiroshima": {"name": "åºƒå³¶çœŒ", "code": "34"},
            "yamaguchi": {"name": "å±±å£çœŒ", "code": "35"},
            "fukuoka": {"name": "ç¦å²¡çœŒ", "code": "40"},
            "osaka": {"name": "å¤§é˜ªåºœ", "code": "27"},
            "tokyo": {"name": "æ±äº¬éƒ½", "code": "13"},
            "okayama": {"name": "å²¡å±±çœŒ", "code": "33"}
        }
        
    def analyze_collected_data(self):
        """åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ"""
        logger.info("åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®åˆ†æä¸­...")
        
        analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "regions": {},
            "data_summary": {
                "total_files": 0,
                "by_region": {},
                "by_type": {}
            }
        }
        
        for region_key, region_info in self.regions.items():
            region_dir = self.data_dir / region_key
            region_analysis = {
                "name": region_info["name"],
                "code": region_info["code"],
                "exists": region_dir.exists(),
                "data_categories": {}
            }
            
            if region_dir.exists():
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                for category_dir in region_dir.iterdir():
                    if category_dir.is_dir():
                        files = list(category_dir.glob("*"))
                        file_count = len(files)
                        
                        if file_count > 0:
                            region_analysis["data_categories"][category_dir.name] = {
                                "file_count": file_count,
                                "sample_files": [f.name for f in files[:3]],
                                "total_size": sum(f.stat().st_size for f in files)
                            }
                            
                            # çµ±è¨ˆæ›´æ–°
                            analysis_results["data_summary"]["total_files"] += file_count
                            
                            if region_key not in analysis_results["data_summary"]["by_region"]:
                                analysis_results["data_summary"]["by_region"][region_key] = 0
                            analysis_results["data_summary"]["by_region"][region_key] += file_count
                            
                            if category_dir.name not in analysis_results["data_summary"]["by_type"]:
                                analysis_results["data_summary"]["by_type"][category_dir.name] = 0
                            analysis_results["data_summary"]["by_type"][category_dir.name] += file_count
                            
            analysis_results["regions"][region_key] = region_analysis
            
        return analysis_results
        
    def integrate_json_data(self):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆ"""
        logger.info("JSONãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        integrated_json = {
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "version": "1.0",
                "regions": list(self.regions.keys())
            },
            "data": {}
        }
        
        # å„åœ°åŸŸã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ
        for region_key in self.regions:
            region_dir = self.data_dir / region_key
            region_data = []
            
            if region_dir.exists():
                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã—ã¦çµ±åˆ
                for json_file in region_dir.rglob("*.json"):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            
                        region_data.append({
                            "file": str(json_file.relative_to(self.data_dir)),
                            "type": self._guess_data_type(json_file),
                            "record_count": self._count_records(data),
                            "sample": self._get_sample_data(data)
                        })
                    except Exception as e:
                        logger.warning(f"JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¹ã‚­ãƒƒãƒ— ({json_file}): {e}")
                        
            if region_data:
                integrated_json["data"][region_key] = region_data
                
        # çµ±åˆçµæœã‚’ä¿å­˜
        output_file = self.integrated_dir / f"integrated_json_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(integrated_json, f, ensure_ascii=False, indent=2)
            
        logger.info(f"JSONçµ±åˆå®Œäº†: {output_file}")
        
        return integrated_json
        
    def _guess_data_type(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬"""
        path_str = str(file_path).lower()
        
        if "population" in path_str or "äººå£" in path_str:
            return "population"
        elif "tourism" in path_str or "è¦³å…‰" in path_str:
            return "tourism"
        elif "transport" in path_str or "bus" in path_str or "railway" in path_str:
            return "transport"
        elif "event" in path_str or "ã‚¤ãƒ™ãƒ³ãƒˆ" in path_str:
            return "events"
        elif "policy" in path_str or "æ”¿ç­–" in path_str:
            return "policy"
        else:
            return "other"
            
    def _count_records(self, data):
        """JSONãƒ‡ãƒ¼ã‚¿ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        if isinstance(data, list):
            return len(data)
        elif isinstance(data, dict):
            # æœ€ã‚‚é•·ã„ãƒªã‚¹ãƒˆã‚’æ¢ã™
            max_length = 1
            for value in data.values():
                if isinstance(value, list):
                    max_length = max(max_length, len(value))
            return max_length
        return 1
        
    def _get_sample_data(self, data):
        """ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—"""
        if isinstance(data, list) and len(data) > 0:
            return data[0] if isinstance(data[0], (dict, list)) else str(data[0])[:100]
        elif isinstance(data, dict):
            # ã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            return list(data.keys())[:10]
        return None
        
    def create_unified_catalog(self, analysis_results):
        """çµ±ä¸€ã‚«ã‚¿ãƒ­ã‚°ã®ä½œæˆ"""
        logger.info("çµ±ä¸€ã‚«ã‚¿ãƒ­ã‚°ä½œæˆä¸­...")
        
        catalog = {
            "title": "Uesugi Engine 6åœ°åŸŸçµ±åˆãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°",
            "description": "åºƒå³¶çœŒã€å±±å£çœŒã€ç¦å²¡çœŒã€å¤§é˜ªåºœã€æ±äº¬éƒ½ã€å²¡å±±çœŒã®çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°",
            "created_at": datetime.now().isoformat(),
            "regions": [],
            "datasets": [],
            "statistics": analysis_results["data_summary"]
        }
        
        # åœ°åŸŸæƒ…å ±ã®è¿½åŠ 
        for region_key, region_analysis in analysis_results["regions"].items():
            region_info = {
                "id": region_key,
                "name": region_analysis["name"],
                "code": region_analysis["code"],
                "has_data": region_analysis["exists"] and len(region_analysis["data_categories"]) > 0,
                "data_categories": list(region_analysis["data_categories"].keys()),
                "file_count": analysis_results["data_summary"]["by_region"].get(region_key, 0)
            }
            catalog["regions"].append(region_info)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã®è¿½åŠ 
            if region_analysis["exists"]:
                for category, category_info in region_analysis["data_categories"].items():
                    dataset = {
                        "region": region_key,
                        "region_name": region_analysis["name"],
                        "category": category,
                        "file_count": category_info["file_count"],
                        "total_size": category_info["total_size"],
                        "sample_files": category_info["sample_files"]
                    }
                    catalog["datasets"].append(dataset)
                    
        # ã‚«ã‚¿ãƒ­ã‚°ä¿å­˜
        catalog_file = self.integrated_dir / f"unified_catalog_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(catalog_file, 'w', encoding='utf-8') as f:
            json.dump(catalog, f, ensure_ascii=False, indent=2)
            
        logger.info(f"çµ±ä¸€ã‚«ã‚¿ãƒ­ã‚°ä½œæˆå®Œäº†: {catalog_file}")
        
        return catalog_file
        
    def generate_integration_report(self, analysis_results, integrated_json, catalog_file):
        """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_lines = [
            "="*80,
            "6åœ°åŸŸãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ",
            "="*80,
            f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}",
            "",
            "ã€åé›†ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘",
            f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {analysis_results['data_summary']['total_files']}",
            "",
            "ã€åœ°åŸŸåˆ¥åé›†çŠ¶æ³ã€‘"
        ]
        
        # åœ°åŸŸåˆ¥çŠ¶æ³
        for region_key, region_analysis in analysis_results["regions"].items():
            status = "âœ…" if region_analysis["exists"] and region_analysis["data_categories"] else "âŒ"
            file_count = analysis_results["data_summary"]["by_region"].get(region_key, 0)
            report_lines.append(f"{status} {region_analysis['name']} ({region_analysis['code']}): {file_count}ãƒ•ã‚¡ã‚¤ãƒ«")
            
            if region_analysis["data_categories"]:
                for category, info in region_analysis["data_categories"].items():
                    report_lines.append(f"    - {category}: {info['file_count']}ãƒ•ã‚¡ã‚¤ãƒ«")
                    
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        report_lines.extend([
            "",
            "ã€ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆã€‘"
        ])
        
        for data_type, count in analysis_results["data_summary"]["by_type"].items():
            report_lines.append(f"- {data_type}: {count}ãƒ•ã‚¡ã‚¤ãƒ«")
            
        # çµ±åˆçµæœ
        report_lines.extend([
            "",
            "ã€çµ±åˆå‡¦ç†çµæœã€‘",
            f"- JSONãƒ‡ãƒ¼ã‚¿çµ±åˆ: {len(integrated_json['data'])}åœ°åŸŸ",
            f"- çµ±ä¸€ã‚«ã‚¿ãƒ­ã‚°: {catalog_file.name}",
            "",
            "ã€æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€‘",
            "1. ä¸è¶³åœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿åé›†",
            "   - å¤§é˜ªåºœã€æ±äº¬éƒ½ã€å²¡å±±çœŒã®å®Ÿãƒ‡ãƒ¼ã‚¿åé›†ãŒå¿…è¦",
            "2. ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ¤œè¨¼",
            "3. PostgreSQLã¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæº–å‚™",
            "4. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰APIã¨ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ",
            "",
            "="*80
        ])
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = self.integrated_dir / f"integration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
            
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print('\n'.join(report_lines))
        
        return report_file
        
    def generate_sql_schema(self):
        """SQL ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆ"""
        logger.info("SQLã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆä¸­...")
        
        schema_sql = f"""-- 6åœ°åŸŸçµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒ
-- Generated: {datetime.now().isoformat()}

-- åœ°åŸŸãƒã‚¹ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS regions (
    region_code VARCHAR(2) PRIMARY KEY,
    region_name VARCHAR(50) NOT NULL,
    region_name_en VARCHAR(50),
    has_data BOOLEAN DEFAULT FALSE,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åœ°åŸŸãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿
INSERT INTO regions (region_code, region_name, region_name_en, has_data) VALUES
    ('34', 'åºƒå³¶çœŒ', 'Hiroshima', TRUE),
    ('35', 'å±±å£çœŒ', 'Yamaguchi', TRUE),
    ('40', 'ç¦å²¡çœŒ', 'Fukuoka', FALSE),
    ('27', 'å¤§é˜ªåºœ', 'Osaka', FALSE),
    ('13', 'æ±äº¬éƒ½', 'Tokyo', FALSE),
    ('33', 'å²¡å±±çœŒ', 'Okayama', FALSE)
ON CONFLICT (region_code) DO UPDATE
    SET has_data = EXCLUDED.has_data,
        last_updated = CURRENT_TIMESTAMP;

-- ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
CREATE TABLE IF NOT EXISTS data_catalog (
    id SERIAL PRIMARY KEY,
    region_code VARCHAR(2) REFERENCES regions(region_code),
    category VARCHAR(50),
    file_name VARCHAR(255),
    file_path TEXT,
    file_size BIGINT,
    record_count INTEGER,
    data_type VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_catalog_region ON data_catalog(region_code);
CREATE INDEX idx_catalog_category ON data_catalog(category);
CREATE INDEX idx_catalog_type ON data_catalog(data_type);

-- ãƒ“ãƒ¥ãƒ¼ã®ä½œæˆ
CREATE OR REPLACE VIEW v_regional_data_summary AS
SELECT 
    r.region_code,
    r.region_name,
    r.has_data,
    COUNT(dc.id) as total_files,
    COALESCE(SUM(dc.file_size), 0) as total_size,
    COALESCE(SUM(dc.record_count), 0) as total_records
FROM regions r
LEFT JOIN data_catalog dc ON r.region_code = dc.region_code
GROUP BY r.region_code, r.region_name, r.has_data
ORDER BY r.region_code;

-- çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ç”¨ã®ã‚¹ãƒˆã‚¢ãƒ‰ãƒ•ã‚¡ãƒ³ã‚¯ã‚·ãƒ§ãƒ³
CREATE OR REPLACE FUNCTION get_regional_data(
    p_region_code VARCHAR(2) DEFAULT NULL,
    p_category VARCHAR(50) DEFAULT NULL,
    p_data_type VARCHAR(50) DEFAULT NULL
)
RETURNS TABLE (
    region_name VARCHAR,
    category VARCHAR,
    file_name VARCHAR,
    record_count INTEGER,
    data_type VARCHAR
)
AS $$
BEGIN
    RETURN QUERY
    SELECT 
        r.region_name,
        dc.category,
        dc.file_name,
        dc.record_count,
        dc.data_type
    FROM data_catalog dc
    JOIN regions r ON dc.region_code = r.region_code
    WHERE 
        (p_region_code IS NULL OR dc.region_code = p_region_code)
        AND (p_category IS NULL OR dc.category = p_category)
        AND (p_data_type IS NULL OR dc.data_type = p_data_type)
    ORDER BY r.region_code, dc.category, dc.file_name;
END;
$$ LANGUAGE plpgsql;
"""
        
        # ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        schema_file = self.integrated_dir / "unified_schema.sql"
        with open(schema_file, 'w', encoding='utf-8') as f:
            f.write(schema_sql)
            
        logger.info(f"SQLã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆå®Œäº†: {schema_file}")
        
        return schema_file


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\nğŸ”„ 6åœ°åŸŸãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ï¼ˆç°¡æ˜“ç‰ˆï¼‰")
    print("="*80)
    
    integrator = SixRegionsDataIntegratorSimple()
    
    try:
        # 1. åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ...")
        analysis_results = integrator.analyze_collected_data()
        
        # 2. JSONãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
        logger.info("\nã‚¹ãƒ†ãƒƒãƒ—2: JSONãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ...")
        integrated_json = integrator.integrate_json_data()
        
        # 3. çµ±ä¸€ã‚«ã‚¿ãƒ­ã‚°ä½œæˆ
        logger.info("\nã‚¹ãƒ†ãƒƒãƒ—3: çµ±ä¸€ã‚«ã‚¿ãƒ­ã‚°ä½œæˆ...")
        catalog_file = integrator.create_unified_catalog(analysis_results)
        
        # 4. SQLã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆ
        logger.info("\nã‚¹ãƒ†ãƒƒãƒ—4: SQLã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆ...")
        schema_file = integrator.generate_sql_schema()
        
        # 5. çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        logger.info("\nã‚¹ãƒ†ãƒƒãƒ—5: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ...")
        report_file = integrator.generate_integration_report(
            analysis_results, integrated_json, catalog_file
        )
        
        print(f"\nâœ… å‡¦ç†å®Œäº†ï¼")
        print(f"çµ±åˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {integrator.integrated_dir}")
        print(f"ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"  - ã‚«ã‚¿ãƒ­ã‚°: {catalog_file.name}")
        print(f"  - ã‚¹ã‚­ãƒ¼ãƒ: {schema_file.name}")
        print(f"  - ãƒ¬ãƒãƒ¼ãƒˆ: {report_file.name}")
        
    except Exception as e:
        logger.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        
    print("\n" + "="*80)


if __name__ == "__main__":
    main()