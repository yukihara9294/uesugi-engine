#!/usr/bin/env python3
"""
6åœ°åŸŸãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦PostgreSQLã«æŠ•å…¥
"""
import json
import pandas as pd
from pathlib import Path
import logging
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
import os

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SixRegionsDataIntegrator:
    """6åœ°åŸŸãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_dir = Path(__file__).parent.parent
        self.data_dir = self.base_dir / "uesugi-engine-data"
        self.integrated_dir = self.data_dir / "6regions_integrated"
        self.integrated_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæƒ…å ±
        self.db_config = {
            "host": os.environ.get("DB_HOST", "localhost"),
            "port": os.environ.get("DB_PORT", "5432"),
            "database": os.environ.get("DB_NAME", "uesugi_db"),
            "user": os.environ.get("DB_USER", "postgres"),
            "password": os.environ.get("DB_PASSWORD", "postgres")
        }
        
        # åœ°åŸŸæƒ…å ±
        self.regions = {
            "hiroshima": {"name": "åºƒå³¶çœŒ", "code": "34"},
            "yamaguchi": {"name": "å±±å£çœŒ", "code": "35"},
            "fukuoka": {"name": "ç¦å²¡çœŒ", "code": "40"},
            "osaka": {"name": "å¤§é˜ªåºœ", "code": "27"},
            "tokyo": {"name": "æ±äº¬éƒ½", "code": "13"},
            "okayama": {"name": "å²¡å±±çœŒ", "code": "33"}
        }
        
    def collect_existing_data(self):
        """æ—¢å­˜ã®åé›†ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª"""
        logger.info("åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªä¸­...")
        
        data_summary = {}
        
        for region_key, region_info in self.regions.items():
            region_dir = self.data_dir / region_key
            data_summary[region_key] = {
                "name": region_info["name"],
                "code": region_info["code"],
                "data_types": {}
            }
            
            if region_dir.exists():
                # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
                categories = ["tourism", "population", "events", "policy", "transport"]
                
                for category in categories:
                    category_dir = region_dir / category
                    if category_dir.exists():
                        files = list(category_dir.glob("*"))
                        data_summary[region_key]["data_types"][category] = {
                            "file_count": len(files),
                            "files": [f.name for f in files[:5]]  # æœ€åˆã®5ãƒ•ã‚¡ã‚¤ãƒ«
                        }
                        
        return data_summary
        
    def integrate_population_data(self):
        """äººå£ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ"""
        logger.info("äººå£ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        integrated_population = []
        
        # å±±å£çœŒã®äººå£ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        yamaguchi_pop_dir = self.data_dir / "yamaguchi" / "population"
        if yamaguchi_pop_dir.exists():
            for csv_file in yamaguchi_pop_dir.glob("*.csv"):
                try:
                    df = pd.read_csv(csv_file, encoding='utf-8')
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¨æ¸¬ã¨æ¨™æº–åŒ–
                    if 'äººå£' in str(csv_file) or 'population' in str(csv_file):
                        # åŸºæœ¬çš„ãªäººå£ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†
                        record = {
                            "region_code": "35",
                            "region_name": "å±±å£çœŒ",
                            "file_name": csv_file.name,
                            "data_type": "population",
                            "row_count": len(df),
                            "columns": list(df.columns)
                        }
                        integrated_population.append(record)
                        
                except Exception as e:
                    logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({csv_file}): {e}")
                    
        # çµ±åˆçµæœã‚’ä¿å­˜
        integration_file = self.integrated_dir / f"integrated_population_{datetime.now().strftime('%Y%m%d')}.json"
        with open(integration_file, 'w', encoding='utf-8') as f:
            json.dump(integrated_population, f, ensure_ascii=False, indent=2)
            
        logger.info(f"äººå£ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(integrated_population)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        return integrated_population
        
    def integrate_transport_data(self):
        """äº¤é€šãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ"""
        logger.info("äº¤é€šãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
        
        integrated_transport = []
        
        # åºƒå³¶çœŒã®GTFSãƒ‡ãƒ¼ã‚¿å‡¦ç†
        hiroshima_gtfs = self.data_dir / "hiroshima" / "transport" / "bus" / "hiroshima_bus_data_20250611.json"
        if hiroshima_gtfs.exists():
            with open(hiroshima_gtfs, 'r', encoding='utf-8') as f:
                gtfs_data = json.load(f)
                
            integrated_transport.append({
                "region_code": "34",
                "region_name": "åºƒå³¶çœŒ",
                "transport_type": "bus",
                "operator": "åºƒå³¶é›»é‰„",
                "routes": gtfs_data.get("routes_count", 0),
                "stops": gtfs_data.get("stops_count", 0),
                "data_source": "GTFS"
            })
            
        # çµ±åˆçµæœã‚’ä¿å­˜
        integration_file = self.integrated_dir / f"integrated_transport_{datetime.now().strftime('%Y%m%d')}.json"
        with open(integration_file, 'w', encoding='utf-8') as f:
            json.dump(integrated_transport, f, ensure_ascii=False, indent=2)
            
        logger.info(f"äº¤é€šãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(integrated_transport)}ä»¶")
        
        return integrated_transport
        
    def integrate_opendata_catalogs(self):
        """ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ã®çµ±åˆ"""
        logger.info("ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°çµ±åˆé–‹å§‹...")
        
        integrated_catalogs = []
        
        # ä¸»è¦éƒ½å¸‚ã®ã‚«ã‚¿ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿
        major_cities_dir = self.data_dir / "major-cities"
        if major_cities_dir.exists():
            for json_file in major_cities_dir.glob("*_opendata_catalog.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        catalog_data = json.load(f)
                        
                    city_name = json_file.stem.replace("_opendata_catalog", "")
                    
                    # åœ°åŸŸã‚³ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°
                    region_mapping = {
                        "tokyo": "13",
                        "osaka": "27",
                        "fukuoka": "40"
                    }
                    
                    if city_name in region_mapping:
                        integrated_catalogs.append({
                            "region_code": region_mapping[city_name],
                            "region_name": self.regions[city_name]["name"],
                            "catalog_source": catalog_data.get("source"),
                            "dataset_count": len(catalog_data.get("datasets", [])),
                            "categories": catalog_data.get("categories", []),
                            "last_updated": catalog_data.get("last_updated")
                        })
                        
                except Exception as e:
                    logger.error(f"ã‚«ã‚¿ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({json_file}): {e}")
                    
        # çµ±åˆçµæœã‚’ä¿å­˜
        integration_file = self.integrated_dir / f"integrated_catalogs_{datetime.now().strftime('%Y%m%d')}.json"
        with open(integration_file, 'w', encoding='utf-8') as f:
            json.dump(integrated_catalogs, f, ensure_ascii=False, indent=2)
            
        logger.info(f"ã‚«ã‚¿ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†: {len(integrated_catalogs)}ä»¶")
        
        return integrated_catalogs
        
    def create_integration_summary(self, results):
        """çµ±åˆçµæœã®ã‚µãƒãƒªãƒ¼ä½œæˆ"""
        summary = {
            "timestamp": datetime.now().isoformat(),
            "integration_results": {
                "population": len(results.get("population", [])),
                "transport": len(results.get("transport", [])),
                "catalogs": len(results.get("catalogs", [])),
                "total_regions_with_data": 0
            },
            "region_coverage": {},
            "next_steps": []
        }
        
        # åœ°åŸŸåˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸
        covered_regions = set()
        
        for data_type, data_list in results.items():
            for item in data_list:
                if "region_code" in item:
                    covered_regions.add(item["region_code"])
                    
        summary["integration_results"]["total_regions_with_data"] = len(covered_regions)
        
        # å„åœ°åŸŸã®ã‚«ãƒãƒ¬ãƒƒã‚¸çŠ¶æ³
        for region_key, region_info in self.regions.items():
            has_data = region_info["code"] in covered_regions
            summary["region_coverage"][region_key] = {
                "name": region_info["name"],
                "has_data": has_data,
                "status": "âœ… ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š" if has_data else "âŒ ãƒ‡ãƒ¼ã‚¿ãªã—"
            }
            
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
        if len(covered_regions) < 6:
            summary["next_steps"].append("ä¸è¶³ã—ã¦ã„ã‚‹åœ°åŸŸã®ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å®Ÿæ–½")
        summary["next_steps"].extend([
            "PostgreSQLã¸ã®ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè¡Œ",
            "ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¨ã®APIæ¥ç¶šãƒ†ã‚¹ãƒˆ",
            "ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ¤œè¨¼"
        ])
        
        # ã‚µãƒãƒªãƒ¼ä¿å­˜
        summary_file = self.integrated_dir / f"integration_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
            
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        self._print_summary_report(summary)
        
        return summary_file
        
    def _print_summary_report(self, summary):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º"""
        print("\n" + "="*70)
        print("6åœ°åŸŸãƒ‡ãƒ¼ã‚¿çµ±åˆã‚µãƒãƒªãƒ¼")
        print("="*70)
        print(f"å®Ÿè¡Œæ™‚åˆ»: {summary['timestamp']}")
        print(f"\nã€çµ±åˆçµæœã€‘")
        print(f"- äººå£ãƒ‡ãƒ¼ã‚¿: {summary['integration_results']['population']}ä»¶")
        print(f"- äº¤é€šãƒ‡ãƒ¼ã‚¿: {summary['integration_results']['transport']}ä»¶")
        print(f"- ã‚«ã‚¿ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿: {summary['integration_results']['catalogs']}ä»¶")
        print(f"- ãƒ‡ãƒ¼ã‚¿ã®ã‚ã‚‹åœ°åŸŸæ•°: {summary['integration_results']['total_regions_with_data']}/6")
        
        print(f"\nã€åœ°åŸŸåˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ã€‘")
        for region, info in summary['region_coverage'].items():
            print(f"- {info['name']}: {info['status']}")
            
        print(f"\nã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘")
        for i, step in enumerate(summary['next_steps'], 1):
            print(f"{i}. {step}")
            
        print("\n" + "="*70)
        
    def setup_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèªä¸­...")
        
        try:
            conn = psycopg2.connect(**self.db_config)
            conn.close()
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {e}")
            logger.info("Dockerç’°å¢ƒã§PostgreSQLãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\nğŸ”„ 6åœ°åŸŸãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†")
    print("="*70)
    
    integrator = SixRegionsDataIntegrator()
    
    # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª")
    data_summary = integrator.collect_existing_data()
    
    print("\nã€åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã€‘")
    for region, info in data_summary.items():
        print(f"\n{info['name']} ({info['code']}):")
        if info['data_types']:
            for category, details in info['data_types'].items():
                print(f"  - {category}: {details['file_count']}ãƒ•ã‚¡ã‚¤ãƒ«")
        else:
            print("  - ãƒ‡ãƒ¼ã‚¿ãªã—")
            
    # 2. ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†
    logger.info("\nã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿çµ±åˆå‡¦ç†")
    
    results = {
        "population": integrator.integrate_population_data(),
        "transport": integrator.integrate_transport_data(),
        "catalogs": integrator.integrate_opendata_catalogs()
    }
    
    # 3. çµ±åˆã‚µãƒãƒªãƒ¼ä½œæˆ
    logger.info("\nã‚¹ãƒ†ãƒƒãƒ—3: çµ±åˆã‚µãƒãƒªãƒ¼ä½œæˆ")
    summary_file = integrator.create_integration_summary(results)
    
    # 4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
    logger.info("\nã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç’°å¢ƒç¢ºèª")
    db_ready = integrator.setup_database()
    
    if db_ready:
        print("\nâœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæº–å‚™å®Œäº†")
    else:
        print("\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§Dockerç’°å¢ƒã‚’èµ·å‹•ã—ã¦ãã ã•ã„:")
        print("  cd ~/projects/uesugi-engine")
        print("  docker-compose up -d")
        
    print(f"\nçµ±åˆçµæœ: {integrator.integrated_dir}")
    print(f"ã‚µãƒãƒªãƒ¼: {summary_file}")
    
    print("\n" + "="*70)


if __name__ == "__main__":
    main()