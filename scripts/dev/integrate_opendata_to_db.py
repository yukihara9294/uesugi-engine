#!/usr/bin/env python3
"""
åé›†ã—ãŸã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'backend', 'app'))

import json
import asyncio
import logging
from datetime import datetime
from pathlib import Path
import asyncpg
from typing import Dict, List
import pandas as pd

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class OpenDataIntegrator:
    """ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆ"""
    
    def __init__(self):
        self.db_config = {
            'host': 'localhost',
            'port': 5432,
            'database': 'uesugi_heatmap',
            'user': 'postgres',
            'password': 'postgres'
        }
        self.data_dir = Path("uesugi-engine-data")
        
    async def integrate_all(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
        logger.info("=== ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹ ===")
        
        # æœ€æ–°ã®åé›†çµæœã‚’èª­ã¿è¾¼ã¿
        latest_result = self._get_latest_collection_result()
        if not latest_result:
            logger.error("åé›†çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
            
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
        conn = await asyncpg.connect(**self.db_config)
        
        try:
            # 1. æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
            await self._integrate_weather_data(conn, latest_result.get('weather', {}))
            
            # 2. åœ°éœ‡ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
            await self._integrate_earthquake_data(conn, latest_result.get('earthquakes', []))
            
            # 3. GTFSãƒ‡ãƒ¼ã‚¿ã®çµ±åˆï¼ˆURLã®ã¿ä¿å­˜ï¼‰
            await self._integrate_gtfs_data(conn, latest_result.get('gtfs', {}))
            
            # 4. ç’°å¢ƒãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆ
            await self._integrate_environmental_data(conn, latest_result.get('environmental', {}))
            
            logger.info("âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†")
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
        finally:
            await conn.close()
            
    def _get_latest_collection_result(self) -> Dict:
        """æœ€æ–°ã®åé›†çµæœã‚’å–å¾—"""
        results_dir = self.data_dir / "collection_results"
        if not results_dir.exists():
            return {}
            
        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        json_files = list(results_dir.glob("free_data_*.json"))
        if not json_files:
            return {}
            
        latest_file = max(json_files, key=lambda p: p.stat().st_mtime)
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            return json.load(f)
            
    async def _integrate_weather_data(self, conn, weather_data: Dict):
        """æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆ"""
        logger.info("æ°—è±¡ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
        
        # weather_dataãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS weather_data (
                id SERIAL PRIMARY KEY,
                city VARCHAR(100),
                prefecture VARCHAR(50),
                lat FLOAT,
                lng FLOAT,
                temperature FLOAT,
                humidity INTEGER,
                precipitation FLOAT,
                weather_code INTEGER,
                wind_speed FLOAT,
                observation_time TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # éƒ½å¸‚åº§æ¨™æƒ…å ±
        city_coords = {
            "åºƒå³¶å¸‚": {"lat": 34.3853, "lng": 132.4553, "pref": "åºƒå³¶çœŒ"},
            "å±±å£å¸‚": {"lat": 34.1859, "lng": 131.4705, "pref": "å±±å£çœŒ"},
            "ç¦å²¡å¸‚": {"lat": 33.5904, "lng": 130.4017, "pref": "ç¦å²¡çœŒ"},
            "å¤§é˜ªå¸‚": {"lat": 34.6937, "lng": 135.5023, "pref": "å¤§é˜ªåºœ"},
            "æ±äº¬éƒ½": {"lat": 35.6762, "lng": 139.6503, "pref": "æ±äº¬éƒ½"}
        }
        
        for city_name, data in weather_data.items():
            if data.get('status') == 'success' and 'current' in data:
                current = data['current']
                coords = city_coords.get(city_name, {})
                
                try:
                    await conn.execute('''
                        INSERT INTO weather_data 
                        (city, prefecture, lat, lng, temperature, humidity, 
                         precipitation, weather_code, wind_speed, observation_time)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    ''',
                        city_name,
                        coords.get('pref', ''),
                        coords.get('lat', 0),
                        coords.get('lng', 0),
                        current.get('temperature_2m', 0),
                        current.get('relative_humidity_2m', 0),
                        current.get('precipitation', 0),
                        current.get('weather_code', 0),
                        current.get('wind_speed_10m', 0),
                        datetime.fromisoformat(current.get('time', datetime.now().isoformat()))
                    )
                    logger.info(f"âœ“ {city_name}ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")
                except Exception as e:
                    logger.error(f"âœ— {city_name}ã®æ°—è±¡ãƒ‡ãƒ¼ã‚¿ä¿å­˜å¤±æ•—: {e}")
                    
    async def _integrate_earthquake_data(self, conn, earthquake_data: List):
        """åœ°éœ‡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆ"""
        logger.info("åœ°éœ‡ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
        
        # earthquake_dataãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS earthquake_data (
                id SERIAL PRIMARY KEY,
                event_id VARCHAR(100) UNIQUE,
                magnitude FLOAT,
                depth INTEGER,
                lat FLOAT,
                lng FLOAT,
                place VARCHAR(200),
                event_time TIMESTAMP,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        for quake in earthquake_data[:20]:  # æœ€æ–°20ä»¶ã®ã¿
            try:
                # åœ°éœ‡ãƒ‡ãƒ¼ã‚¿ã®è§£æï¼ˆJMAãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
                event_id = quake.get('eventId', '')
                magnitude = float(quake.get('magnitude', 0)) if quake.get('magnitude') else None
                
                # éœ‡æºæƒ…å ±
                hypocenter = quake.get('hypocenter', {})
                depth = hypocenter.get('depth', 0)
                lat = hypocenter.get('latitude', 0)
                lng = hypocenter.get('longitude', 0)
                place = hypocenter.get('name', '')
                
                # ç™ºç”Ÿæ™‚åˆ»
                origin_time = quake.get('originTime', '')
                if origin_time:
                    event_time = datetime.fromisoformat(origin_time.replace('Z', '+00:00'))
                else:
                    event_time = datetime.now()
                
                await conn.execute('''
                    INSERT INTO earthquake_data 
                    (event_id, magnitude, depth, lat, lng, place, event_time)
                    VALUES ($1, $2, $3, $4, $5, $6, $7)
                    ON CONFLICT (event_id) DO NOTHING
                ''',
                    event_id, magnitude, depth, lat, lng, place, event_time
                )
                
            except Exception as e:
                logger.error(f"åœ°éœ‡ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                
        logger.info(f"âœ“ {len(earthquake_data[:20])}ä»¶ã®åœ°éœ‡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")
        
    async def _integrate_gtfs_data(self, conn, gtfs_data: Dict):
        """GTFSãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆ"""
        logger.info("GTFSãƒ‡ãƒ¼ã‚¿æƒ…å ±çµ±åˆä¸­...")
        
        # gtfs_feedsãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS gtfs_feeds (
                id SERIAL PRIMARY KEY,
                operator_name VARCHAR(100) UNIQUE,
                feed_url TEXT,
                feed_type VARCHAR(100),
                status VARCHAR(50),
                last_checked TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        for operator, info in gtfs_data.items():
            if info.get('status') == 'available':
                try:
                    await conn.execute('''
                        INSERT INTO gtfs_feeds 
                        (operator_name, feed_url, feed_type, status)
                        VALUES ($1, $2, $3, $4)
                        ON CONFLICT (operator_name) 
                        DO UPDATE SET 
                            feed_url = EXCLUDED.feed_url,
                            status = EXCLUDED.status,
                            last_checked = CURRENT_TIMESTAMP
                    ''',
                        operator,
                        info.get('url', ''),
                        info.get('type', ''),
                        'available'
                    )
                    logger.info(f"âœ“ {operator}ã®GTFSæƒ…å ±ã‚’ä¿å­˜")
                except Exception as e:
                    logger.error(f"âœ— {operator}ã®GTFSæƒ…å ±ä¿å­˜å¤±æ•—: {e}")
                    
    async def _integrate_environmental_data(self, conn, env_data: Dict):
        """ç’°å¢ƒãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµ±åˆ"""
        logger.info("ç’°å¢ƒãƒ‡ãƒ¼ã‚¿æƒ…å ±çµ±åˆä¸­...")
        
        # environmental_sourcesãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS environmental_sources (
                id SERIAL PRIMARY KEY,
                category VARCHAR(100),
                name VARCHAR(200),
                description TEXT,
                url TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        for category, sources in env_data.items():
            if isinstance(sources, dict):
                for source_name, source_info in sources.items():
                    try:
                        await conn.execute('''
                            INSERT INTO environmental_sources 
                            (category, name, description, url)
                            VALUES ($1, $2, $3, $4)
                        ''',
                            category,
                            source_name,
                            source_info.get('description', ''),
                            source_info.get('url', '')
                        )
                        logger.info(f"âœ“ {category} - {source_name}ã®æƒ…å ±ã‚’ä¿å­˜")
                    except Exception as e:
                        logger.error(f"ç’°å¢ƒãƒ‡ãƒ¼ã‚¿æƒ…å ±ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                        

async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    integrator = OpenDataIntegrator()
    await integrator.integrate_all()
    
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆå®Œäº†")
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ãƒªã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º")
    print("2. APIã‚­ãƒ¼ã‚’å–å¾—ã—ãŸã‚‰è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
    print("3. å®šæœŸçš„ãªæ›´æ–°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®š")


if __name__ == "__main__":
    asyncio.run(main())