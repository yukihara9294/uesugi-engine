#!/usr/bin/env python3
"""
å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ« ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ç™»éŒ²ä¸è¦ã§åˆ©ç”¨å¯èƒ½
"""
import requests
import json
from datetime import datetime
from pathlib import Path
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class YamaguchiOpenDataCollector:
    """å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†"""
    
    def __init__(self):
        self.base_url = "https://yamaguchi-opendata.jp/ckan/api/3/action"
        self.data_dir = Path("uesugi-engine-data/yamaguchi")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
    def get_package_list(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã‚’å–å¾—"""
        url = f"{self.base_url}/package_list"
        
        try:
            response = requests.get(url)
            response.raise_for_status()
            data = response.json()
            
            if data["success"]:
                return data["result"]
            else:
                logger.error("ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—")
                return []
                
        except Exception as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return []
            
    def search_packages(self, query):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¤œç´¢"""
        url = f"{self.base_url}/package_search"
        params = {
            "q": query,
            "rows": 100
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            if data["success"]:
                return data["result"]["results"]
            else:
                return []
                
        except Exception as e:
            logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
            
    def get_package_details(self, package_id):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        url = f"{self.base_url}/package_show"
        params = {"id": package_id}
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            if data["success"]:
                return data["result"]
            else:
                return None
                
        except Exception as e:
            logger.error(f"è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
            
    def download_resource(self, resource_url, filename):
        """ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            response = requests.get(resource_url, stream=True)
            response.raise_for_status()
            
            file_path = self.data_dir / filename
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    
            logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            return None
            
    def collect_tourism_data(self):
        """è¦³å…‰é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("è¦³å…‰é–¢é€£ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        
        # è¦³å…‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
        keywords = ["è¦³å…‰", "å®¿æ³Š", "ã‚¤ãƒ™ãƒ³ãƒˆ", "è¦³å…‰å®¢", "æ¥è¨ªè€…"]
        tourism_datasets = []
        
        for keyword in keywords:
            logger.info(f"'{keyword}'ã§æ¤œç´¢ä¸­...")
            results = self.search_packages(keyword)
            
            for dataset in results:
                dataset_info = {
                    "id": dataset["id"],
                    "name": dataset["name"],
                    "title": dataset["title"],
                    "notes": dataset.get("notes", ""),
                    "resources": []
                }
                
                # ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
                for resource in dataset.get("resources", []):
                    if resource["format"].upper() in ["CSV", "JSON", "XLS", "XLSX"]:
                        dataset_info["resources"].append({
                            "name": resource["name"],
                            "format": resource["format"],
                            "url": resource["url"],
                            "size": resource.get("size", "unknown")
                        })
                        
                if dataset_info["resources"]:
                    tourism_datasets.append(dataset_info)
                    
        logger.info(f"è¦³å…‰é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(tourism_datasets)}ä»¶")
        return tourism_datasets
        
    def collect_population_data(self):
        """äººå£çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("äººå£çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        
        population_datasets = []
        results = self.search_packages("äººå£")
        
        for dataset in results:
            dataset_info = {
                "id": dataset["id"],
                "name": dataset["name"],
                "title": dataset["title"],
                "resources": []
            }
            
            # CSV/Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡
            for resource in dataset.get("resources", []):
                if resource["format"].upper() in ["CSV", "XLS", "XLSX"]:
                    dataset_info["resources"].append({
                        "name": resource["name"],
                        "format": resource["format"],
                        "url": resource["url"]
                    })
                    
            if dataset_info["resources"]:
                population_datasets.append(dataset_info)
                
        logger.info(f"äººå£çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(population_datasets)}ä»¶")
        return population_datasets
        
    def collect_event_data(self):
        """ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’åé›†"""
        logger.info("ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±åé›†é–‹å§‹")
        
        event_datasets = []
        keywords = ["ã‚¤ãƒ™ãƒ³ãƒˆ", "ç¥­ã‚Š", "å‚¬ã—", "è¡Œäº‹"]
        
        for keyword in keywords:
            results = self.search_packages(keyword)
            
            for dataset in results:
                if dataset["id"] not in [d["id"] for d in event_datasets]:
                    event_datasets.append({
                        "id": dataset["id"],
                        "name": dataset["name"], 
                        "title": dataset["title"],
                        "notes": dataset.get("notes", "")
                    })
                    
        logger.info(f"ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(event_datasets)}ä»¶")
        return event_datasets
        
    def save_summary(self, all_data):
        """åé›†çµæœã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONä¿å­˜
        json_path = self.data_dir / f"yamaguchi_summary_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
            
        # ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒãƒªãƒ¼
        summary_path = self.data_dir / f"yamaguchi_summary_{timestamp}.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†ã‚µãƒãƒªãƒ¼\n")
            f.write("="*60 + "\n")
            f.write(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now()}\n")
            f.write(f"APIã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: âœ… ç™»éŒ²ä¸è¦ãƒ»åˆ©ç”¨å¯èƒ½\n\n")
            
            # è¦³å…‰ãƒ‡ãƒ¼ã‚¿
            f.write(f"ã€è¦³å…‰é–¢é€£ãƒ‡ãƒ¼ã‚¿ã€‘\n")
            f.write(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(all_data['tourism'])}ä»¶\n")
            for dataset in all_data['tourism'][:5]:
                f.write(f"- {dataset['title']}\n")
                
            f.write(f"\nã€äººå£çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã€‘\n")
            f.write(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(all_data['population'])}ä»¶\n")
            for dataset in all_data['population'][:5]:
                f.write(f"- {dataset['title']}\n")
                
            f.write(f"\nã€ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€‘\n")
            f.write(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(all_data['events'])}ä»¶\n")
            for dataset in all_data['events'][:5]:
                f.write(f"- {dataset['title']}\n")
                
        logger.info(f"ã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_path}")
        return json_path, summary_path


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ›ï¸ å±±å£çœŒã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
    print("="*60)
    print("âœ… APIã‚­ãƒ¼ä¸è¦ãƒ»ç™»éŒ²ä¸è¦ã§åˆ©ç”¨å¯èƒ½ï¼")
    print()
    
    collector = YamaguchiOpenDataCollector()
    
    # ãƒ‡ãƒ¼ã‚¿åé›†
    all_data = {
        "timestamp": datetime.now().isoformat(),
        "tourism": collector.collect_tourism_data(),
        "population": collector.collect_population_data(),
        "events": collector.collect_event_data()
    }
    
    # ã‚µãƒãƒªãƒ¼ä¿å­˜
    json_path, summary_path = collector.save_summary(all_data)
    
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†ï¼")
    print(f"JSONä¿å­˜å…ˆ: {json_path}")
    print(f"ã‚µãƒãƒªãƒ¼: {summary_path}")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    with open(summary_path, 'r', encoding='utf-8') as f:
        print("\n" + f.read())
        
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒªã‚½ãƒ¼ã‚¹æ•°ã‚’è¡¨ç¤º
    total_resources = sum(len(d.get("resources", [])) for d in all_data["tourism"])
    print(f"\nğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒªã‚½ãƒ¼ã‚¹: {total_resources}ä»¶")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®1ä»¶ï¼‰
    if all_data["tourism"] and all_data["tourism"][0]["resources"]:
        first_resource = all_data["tourism"][0]["resources"][0]
        print(f"\nã‚µãƒ³ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {first_resource['name']}")
        # å®Ÿéš›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–ï¼‰
        # collector.download_resource(first_resource['url'], f"sample_{first_resource['name']}")


if __name__ == "__main__":
    main()