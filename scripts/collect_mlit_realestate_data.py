#!/usr/bin/env python3
"""
å›½åœŸäº¤é€šçœãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ»ä¸å‹•ç”£æƒ…å ±ãƒ©ã‚¤ãƒ–ãƒ©ãƒªAPIçµ±åˆ
Uesugi Engineç”¨ã®ä¸å‹•ç”£ãƒ»åœ°åŸŸãƒ‡ãƒ¼ã‚¿åé›†
"""
import requests
import json
from datetime import datetime
from pathlib import Path
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MLITDataCollector:
    """å›½åœŸäº¤é€šçœãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.data_dir = Path("uesugi-engine-data/mlit")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # APIåŸºæœ¬æƒ…å ±
        self.mlit_dpf_base = "https://www.mlit-data.jp/api"
        self.reinfolib_base = "https://www.reinfolib.mlit.go.jp/api"
        
        # å¯¾è±¡åœ°åŸŸï¼ˆUesugi Engineå¯¾è±¡ï¼‰
        self.target_prefectures = {
            "34": "åºƒå³¶çœŒ",
            "35": "å±±å£çœŒ", 
            "40": "ç¦å²¡çœŒ",
            "27": "å¤§é˜ªåºœ",
            "13": "æ±äº¬éƒ½"
        }
        
    def get_prefecture_list(self):
        """éƒ½é“åºœçœŒä¸€è¦§å–å¾—"""
        logger.info("=== éƒ½é“åºœçœŒæƒ…å ±å–å¾— ===")
        
        try:
            # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰æƒ…å ±å–å¾—API
            url = f"{self.mlit_dpf_base}/v1/prefectures"
            response = requests.get(url)
            
            if response.status_code == 200:
                prefectures = response.json()
                
                # å¯¾è±¡åœ°åŸŸã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
                target_data = {
                    code: name for code, name in prefectures.items() 
                    if code in self.target_prefectures
                }
                
                # ä¿å­˜
                output_path = self.data_dir / "prefecture_codes.json"
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        "timestamp": datetime.now().isoformat(),
                        "source": "å›½åœŸäº¤é€šçœDPF",
                        "prefectures": target_data
                    }, f, ensure_ascii=False, indent=2)
                    
                logger.info(f"âœ“ éƒ½é“åºœçœŒæƒ…å ±ä¿å­˜: {output_path}")
                return target_data
                
        except Exception as e:
            logger.error(f"éƒ½é“åºœçœŒæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return self.target_prefectures
            
    def get_city_list(self, prefecture_code):
        """å¸‚åŒºç”ºæ‘ä¸€è¦§å–å¾—"""
        logger.info(f"=== {self.target_prefectures.get(prefecture_code, prefecture_code)}ã®å¸‚åŒºç”ºæ‘æƒ…å ±å–å¾— ===")
        
        try:
            # å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰æƒ…å ±å–å¾—API
            url = f"{self.mlit_dpf_base}/v1/cities"
            params = {"prefecture": prefecture_code}
            response = requests.get(url, params=params)
            
            if response.status_code == 200:
                cities = response.json()
                
                # ä¿å­˜
                output_path = self.data_dir / f"cities_{prefecture_code}.json"
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        "timestamp": datetime.now().isoformat(),
                        "prefecture": self.target_prefectures.get(prefecture_code),
                        "cities": cities
                    }, f, ensure_ascii=False, indent=2)
                    
                logger.info(f"âœ“ {len(cities)}å¸‚åŒºç”ºæ‘ã®æƒ…å ±ä¿å­˜")
                return cities
                
        except Exception as e:
            logger.error(f"å¸‚åŒºç”ºæ‘æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
            
    def get_real_estate_prices(self, prefecture_code, city_code=None):
        """ä¸å‹•ç”£å–å¼•ä¾¡æ ¼æƒ…å ±å–å¾—"""
        logger.info(f"=== ä¸å‹•ç”£å–å¼•ä¾¡æ ¼æƒ…å ±å–å¾— ===")
        
        try:
            # ä¸å‹•ç”£å–å¼•ä¾¡æ ¼æƒ…å ±å–å¾—API
            url = f"{self.reinfolib_base}/webapi/v2/TransactionSearch"
            
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ2023å¹´ç¬¬3å››åŠæœŸï½2024å¹´ç¬¬1å››åŠæœŸï¼‰
            params = {
                "from": "20233",
                "to": "20241",
                "prefCode": prefecture_code
            }
            
            if city_code:
                params["cityCode"] = city_code
                
            response = requests.get(url, params=params)
            
            if response.status_code == 200:
                data = response.json()
                transactions = data.get("results", [])
                
                # ãƒ‡ãƒ¼ã‚¿é›†è¨ˆ
                summary = {
                    "total_count": len(transactions),
                    "average_price": 0,
                    "price_range": {"min": 0, "max": 0},
                    "type_distribution": {},
                    "area_distribution": {}
                }
                
                if transactions:
                    prices = [int(t.get("TradePrice", 0)) for t in transactions if t.get("TradePrice")]
                    if prices:
                        summary["average_price"] = sum(prices) / len(prices)
                        summary["price_range"]["min"] = min(prices)
                        summary["price_range"]["max"] = max(prices)
                        
                    # ç‰©ä»¶ç¨®åˆ¥é›†è¨ˆ
                    for t in transactions:
                        prop_type = t.get("Type", "ä¸æ˜")
                        summary["type_distribution"][prop_type] = summary["type_distribution"].get(prop_type, 0) + 1
                        
                        # åœ°åŸŸåˆ¥é›†è¨ˆ
                        area = t.get("Municipality", "ä¸æ˜")
                        summary["area_distribution"][area] = summary["area_distribution"].get(area, 0) + 1
                
                # ä¿å­˜
                output_path = self.data_dir / f"real_estate_prices_{prefecture_code}.json"
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        "timestamp": datetime.now().isoformat(),
                        "prefecture": self.target_prefectures.get(prefecture_code),
                        "period": "2023Q3-2024Q1",
                        "summary": summary,
                        "sample_data": transactions[:5]  # ã‚µãƒ³ãƒ—ãƒ«5ä»¶
                    }, f, ensure_ascii=False, indent=2)
                    
                logger.info(f"âœ“ {summary['total_count']}ä»¶ã®å–å¼•ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ")
                return summary
                
        except Exception as e:
            logger.error(f"ä¸å‹•ç”£ä¾¡æ ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
            
    def analyze_policy_impact(self):
        """æ–½ç­–å½±éŸ¿åˆ†æã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿çµ±åˆ"""
        logger.info("=== æ–½ç­–å½±éŸ¿åˆ†æãƒ‡ãƒ¼ã‚¿æº–å‚™ ===")
        
        analysis_framework = {
            "ä¸å‹•ç”£ä¾¡æ ¼å¤‰å‹•åˆ†æ": {
                "ç”¨é€”": "ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™ãƒ»éƒ½å¸‚é–‹ç™ºã®åŠ¹æœæ¸¬å®š",
                "æŒ‡æ¨™": [
                    "æ–°é§…é–‹æ¥­å‰å¾Œã®åœ°ä¾¡å¤‰å‹•",
                    "å†é–‹ç™ºåœ°åŸŸã®ä¸å‹•ç”£ä¾¡æ ¼æ¨ç§»",
                    "ç½å®³ãƒªã‚¹ã‚¯åœ°åŸŸã®ä¾¡æ ¼å½±éŸ¿"
                ],
                "æ‰‹æ³•": "DIDï¼ˆå·®åˆ†ã®å·®åˆ†æ³•ï¼‰ã€åˆæˆã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«"
            },
            "åœ°åŸŸçµŒæ¸ˆåˆ†æ": {
                "ç”¨é€”": "ç”£æ¥­æ”¿ç­–ãƒ»ä¼æ¥­èª˜è‡´åŠ¹æœ",
                "æŒ‡æ¨™": [
                    "äº‹æ¥­æ‰€æ•°ãƒ»å¾“æ¥­è€…æ•°ã®å¤‰åŒ–",
                    "ä¸€äººå½“ãŸã‚Šåœ°æ–¹ç¨ã®æ¨ç§»",
                    "ä¼æ¥­ç«‹åœ°ã¨é›‡ç”¨å‰µå‡º"
                ],
                "æ‰‹æ³•": "æ™‚ç³»åˆ—åˆ†æã€ãƒ‘ãƒãƒ«ãƒ‡ãƒ¼ã‚¿åˆ†æ"
            },
            "äººå£å‹•æ…‹åˆ†æ": {
                "ç”¨é€”": "ç§»ä½ãƒ»å®šä½æ”¿ç­–åŠ¹æœ",
                "æŒ‡æ¨™": [
                    "å¹´é½¢åˆ¥äººå£æ§‹æˆã®å¤‰åŒ–",
                    "è»¢å…¥è»¢å‡ºãƒãƒ©ãƒ³ã‚¹",
                    "ä¸–å¸¯æ•°ãƒ»ä¸–å¸¯æ§‹æˆ"
                ],
                "æ‰‹æ³•": "ã‚³ãƒ¼ãƒ›ãƒ¼ãƒˆåˆ†æã€äººå£äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"
            },
            "çµ±åˆåˆ†æ": {
                "ç”¨é€”": "è¤‡åˆçš„ãªæ–½ç­–åŠ¹æœæ¸¬å®š",
                "ä¾‹": [
                    "äº¤é€šæ•´å‚™â†’åœ°ä¾¡ä¸Šæ˜‡â†’ç¨åå¢—åŠ ",
                    "ä¼æ¥­èª˜è‡´â†’é›‡ç”¨å‰µå‡ºâ†’äººå£å¢—åŠ ",
                    "é˜²ç½æŠ•è³‡â†’ãƒªã‚¹ã‚¯ä½æ¸›â†’åœ°åŸŸæ´»æ€§åŒ–"
                ]
            }
        }
        
        # åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ä¿å­˜
        output_path = self.data_dir / "policy_impact_analysis_framework.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "framework": analysis_framework,
                "data_sources": {
                    "å›½åœŸäº¤é€šçœDPF": ["éƒ½é“åºœçœŒãƒ»å¸‚åŒºç”ºæ‘ã‚³ãƒ¼ãƒ‰", "åœ°ç†æƒ…å ±"],
                    "ä¸å‹•ç”£æƒ…å ±ãƒ©ã‚¤ãƒ–ãƒ©ãƒª": ["å–å¼•ä¾¡æ ¼", "åœ°ä¾¡å…¬ç¤º"],
                    "e-Stat": ["äººå£", "äº‹æ¥­æ‰€", "åœ°æ–¹è²¡æ”¿"]
                }
            }, f, ensure_ascii=False, indent=2)
            
        logger.info(f"âœ“ åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ä¿å­˜: {output_path}")
        return analysis_framework


class EStatAdvancedCollector:
    """e-Statè©³ç´°ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆæ—¢å­˜APIã‚­ãƒ¼æ´»ç”¨ï¼‰"""
    
    def __init__(self):
        self.api_key = "c11c2e7910b7810c15770f829b52bb1a75d283ed"
        self.base_url = "https://api.e-stat.go.jp/rest/3.0/app/json"
        self.data_dir = Path("uesugi-engine-data/estat-advanced")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
    def get_local_tax_data(self):
        """ä¸€äººå½“ãŸã‚Šåœ°æ–¹ç¨ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        logger.info("=== åœ°æ–¹ç¨ãƒ‡ãƒ¼ã‚¿å–å¾— ===")
        
        # åœ°æ–¹è²¡æ”¿çŠ¶æ³èª¿æŸ»ã®statsDataId
        stats_data_id = "0004006284"  # 2023å¹´å¸‚ç”ºæ‘åˆ†
        
        url = f"{self.base_url}/getStatsData"
        params = {
            "appId": self.api_key,
            "lang": "J",
            "statsDataId": stats_data_id,
            "metaGetFlg": "Y",
            "cntGetFlg": "N"
        }
        
        try:
            response = requests.get(url, params=params)
            if response.status_code == 200:
                data = response.json()
                logger.info("âœ“ åœ°æ–¹ç¨ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                
                # ä¿å­˜
                output_path = self.data_dir / "local_tax_per_capita.json"
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
                return data
                
        except Exception as e:
            logger.error(f"åœ°æ–¹ç¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
            
    def get_business_census_data(self):
        """çµŒæ¸ˆã‚»ãƒ³ã‚µã‚¹äº‹æ¥­æ‰€ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        logger.info("=== äº‹æ¥­æ‰€ãƒ»ä¼æ¥­ãƒ‡ãƒ¼ã‚¿å–å¾— ===")
        
        # ä»¤å’Œ3å¹´çµŒæ¸ˆã‚»ãƒ³ã‚µã‚¹ã®ID
        stats_data_id = "0003448237"
        
        url = f"{self.base_url}/getStatsData"
        params = {
            "appId": self.api_key,
            "lang": "J",
            "statsDataId": stats_data_id,
            "metaGetFlg": "Y",
            "cntGetFlg": "N",
            "limit": 1000
        }
        
        try:
            response = requests.get(url, params=params)
            if response.status_code == 200:
                data = response.json()
                logger.info("âœ“ äº‹æ¥­æ‰€ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                
                # ä¿å­˜
                output_path = self.data_dir / "business_census_2021.json"
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
                return data
                
        except Exception as e:
            logger.error(f"äº‹æ¥­æ‰€ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ›ï¸ å›½åœŸäº¤é€šçœãƒ»e-Statçµ±åˆãƒ‡ãƒ¼ã‚¿åé›†")
    print("="*60)
    
    # 1. å›½åœŸäº¤é€šçœãƒ‡ãƒ¼ã‚¿åé›†
    print("\nã€å›½åœŸäº¤é€šçœãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€‘")
    mlit_collector = MLITDataCollector()
    
    # éƒ½é“åºœçœŒæƒ…å ±
    print("\nğŸ“ éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰å–å¾—...")
    prefectures = mlit_collector.get_prefecture_list()
    
    # å„éƒ½é“åºœçœŒã®å¸‚åŒºç”ºæ‘æƒ…å ±
    print("\nğŸ˜ï¸ å¸‚åŒºç”ºæ‘æƒ…å ±å–å¾—...")
    for pref_code in mlit_collector.target_prefectures:
        cities = mlit_collector.get_city_list(pref_code)
        
    # ä¸å‹•ç”£ä¾¡æ ¼æƒ…å ±ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼šåºƒå³¶çœŒï¼‰
    print("\nğŸ  ä¸å‹•ç”£å–å¼•ä¾¡æ ¼æƒ…å ±å–å¾—...")
    real_estate_summary = mlit_collector.get_real_estate_prices("34")
    
    # æ–½ç­–å½±éŸ¿åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
    print("\nğŸ“Š æ–½ç­–å½±éŸ¿åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ä½œæˆ...")
    analysis = mlit_collector.analyze_policy_impact()
    
    # 2. e-Statè©³ç´°ãƒ‡ãƒ¼ã‚¿åé›†
    print("\nã€e-Statè©³ç´°ãƒ‡ãƒ¼ã‚¿ã€‘")
    estat_collector = EStatAdvancedCollector()
    
    print("\nğŸ’° åœ°æ–¹ç¨ãƒ‡ãƒ¼ã‚¿å–å¾—...")
    tax_data = estat_collector.get_local_tax_data()
    
    print("\nğŸ¢ äº‹æ¥­æ‰€ãƒ»ä¼æ¥­ãƒ‡ãƒ¼ã‚¿å–å¾—...")
    business_data = estat_collector.get_business_census_data()
    
    print("\nâœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†ï¼")
    print("\n" + "="*60)
    print("ã€Uesugi Engineã§ã®æ´»ç”¨ã€‘")
    print("\n1. ä¸å‹•ç”£ä¾¡æ ¼åˆ†æ")
    print("   â†’ ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™åŠ¹æœã®å®šé‡åŒ–")
    print("   â†’ éƒ½å¸‚é–‹ç™ºã®çµŒæ¸ˆåŠ¹æœæ¸¬å®š")
    
    print("\n2. åœ°åŸŸçµŒæ¸ˆåˆ†æ")
    print("   â†’ ä¼æ¥­èª˜è‡´æ”¿ç­–ã®åŠ¹æœæ¤œè¨¼")
    print("   â†’ é›‡ç”¨å‰µå‡ºã¨ç¨åã®ç›¸é–¢åˆ†æ")
    
    print("\n3. è¤‡åˆçš„ãªå› æœæ¨è«–")
    print("   â†’ äº¤é€šÃ—ä¸å‹•ç”£Ã—äººå£ã®é€£é–åŠ¹æœ")
    print("   â†’ æ”¿ç­–ã®æ³¢åŠåŠ¹æœã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    
    print("\nğŸ’¡ ã“ã‚Œã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚Šã€ã‚ˆã‚Šç²¾å¯†ãªæ”¿ç­–åŠ¹æœæ¸¬å®šãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼")


if __name__ == "__main__":
    main()